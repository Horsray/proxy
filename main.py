#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»˜å½±AICGä»£ç†æœåŠ¡å™¨ä¸»ç¨‹åº

è¯¥ä»£ç†æœåŠ¡å™¨è´Ÿè´£è¿æ¥ç»˜å½±AIå‰ç«¯ä¸ComfyUIåç«¯ï¼Œæä¾›ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š
- ç”¨æˆ·è®¤è¯ä¸ä¼šè¯ç®¡ç†ï¼ˆæ”¯æŒæœ¬åœ°å’Œäº‘ç«¯éªŒè¯ï¼‰
- å·¥ä½œæµåŠ è½½ã€å‚æ•°æ˜ å°„ä¸åˆå¹¶å¤„ç†
- ä»»åŠ¡æäº¤ã€è¿›åº¦è·Ÿè¸ªä¸çŠ¶æ€ç®¡ç†
- WebSocketé€šä¿¡ä¸æ¶ˆæ¯é˜Ÿåˆ—ç®¡ç†
- å®¢æˆ·ç«¯è¿æ¥ç»´æŠ¤ä¸èµ„æºè‡ªåŠ¨æ¸…ç†

ç‰ˆæœ¬: 2.5.0
"""
import os
import json
import time
import uuid
import logging
from collections import defaultdict, deque
import requests
from flask import Flask, request, jsonify, Response, session
from flask_cors import CORS
import gevent
from gevent import monkey
from gevent.pywsgi import WSGIServer
from geventwebsocket.handler import WebSocketHandler
import threading

monkey.patch_all()

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# ç”¨æˆ·æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå­˜å‚¨æœ¬åœ°è®¤è¯ç”¨æˆ·ä¿¡æ¯ï¼‰
USERS_FILE = "users.json"
# æœ¬åœ°ç”¨æˆ·ç¼“å­˜: {username: {password, ...}} - åŠ è½½è‡ªUSERS_FILE
LOCAL_USERS = {}
# æ´»è·ƒä¼šè¯å­˜å‚¨: {token: username} - ç”¨äºéªŒè¯ç”¨æˆ·åœ¨çº¿çŠ¶æ€
# é”®ä¸ºè®¤è¯ä»¤ç‰Œï¼Œå€¼ä¸ºç”¨æˆ·å
sessions = {}

# è®°å½•æ¯ä¸ªç”¨æˆ·çš„æœ€æ–° token
user_latest_token = {}
# è®°å½•æ¯ä¸ªç”¨æˆ·çš„æœ€æ–° headers
user_latest_headers = {}

def load_local_users():
    """
    åŠ è½½æœ¬åœ°ç”¨æˆ·æ•°æ®åˆ°LOCAL_USERSå…¨å±€å˜é‡
    ä»USERS_FILEæŒ‡å®šçš„JSONæ–‡ä»¶è¯»å–ç”¨æˆ·ä¿¡æ¯ï¼Œæ”¯æŒæœ¬åœ°è®¤è¯
    å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥ï¼Œå°†åˆå§‹åŒ–ç©ºç”¨æˆ·å­—å…¸
    
    è¿”å›:
        dict: åŠ è½½æˆåŠŸçš„ç”¨æˆ·å­—å…¸ï¼Œæ ¼å¼ä¸º{username: {password, ...}}
    """
    global LOCAL_USERS
    try:
        if os.path.exists(USERS_FILE):
            with open(USERS_FILE, 'r', encoding='utf-8') as f:
                # users.json ç»“æ„ä¸º {"users": {username: info}}
                LOCAL_USERS = json.load(f).get("users", {})
                logger.info(
                    f"ğŸ“¦ å·²åŠ è½½æœ¬åœ°ç”¨æˆ·: {list(LOCAL_USERS.keys())}"
                )
        else:
            logger.warning(f"âš ï¸ ç”¨æˆ·æ–‡ä»¶ä¸å­˜åœ¨: {USERS_FILE}")
            LOCAL_USERS = {}
    except Exception as e:
        logger.error(f"âŒ åŠ è½½æœ¬åœ°ç”¨æˆ·å¤±è´¥: {e}")
        LOCAL_USERS = {}
    return LOCAL_USERS


def save_local_users():
    """å°†å½“å‰ LOCAL_USERS å†™å› USERS_FILE"""
    try:
        with open(USERS_FILE, "w", encoding="utf-8") as f:
            json.dump({"users": LOCAL_USERS}, f, indent=4, ensure_ascii=False)
            logger.info("ğŸ’¾ æœ¬åœ°ç”¨æˆ·æ•°æ®å·²ä¿å­˜")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜æœ¬åœ°ç”¨æˆ·å¤±è´¥: {e}")

# åˆå§‹åŒ–æœ¬åœ°ç”¨æˆ·æ•°æ®å°†åœ¨æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–åæ‰§è¡Œ

from datetime import datetime, timedelta
from threading import Thread, Lock
from flask import Flask, request, jsonify, Response
from flask_cors import CORS
import websocket as ws_client
from collections import defaultdict, deque
# Default ComfyUI URL, will be overwritten by config on start
COMFYUI_URL = ""

logging.basicConfig(
    level=logging.DEBUG, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('huiying_proxy.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

LOCAL_USERS = load_local_users()


app = Flask(__name__)
CORS(app, origins="*")


def sanitize_url(url: str) -> str:
    """Normalize user-provided URLs for requests."""
    if not url:
        return url
    return url.replace("\\", "/").rstrip('/')


def is_remote_url(url: str) -> bool:
    """Return True if the given URL points to a public (non-local) address."""
    try:
        from urllib.parse import urlparse
        import ipaddress

        host = urlparse(url).hostname
        if not host:
            return False
        try:
            ip = ipaddress.ip_address(host)
            return not (ip.is_private or ip.is_loopback)
        except ValueError:
            # Host is not an IP address; assume remote
            return True
    except Exception:
        return False


def adapt_workflow_paths(workflow_data, comfyui_url: str):
    """Convert path separators for remote ComfyUI servers."""
    if not is_remote_url(comfyui_url):
        return workflow_data

    def _convert(obj):
        if isinstance(obj, dict):
            return {k: _convert(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [_convert(v) for v in obj]
        elif isinstance(obj, str):
            return obj.replace("\\", "/")
        else:
            return obj

    return _convert(workflow_data)


comfyui_ws = None  
# æ¶ˆæ¯é˜Ÿåˆ—ç®¡ç† - ç”¨äºåœ¨å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨é—´ä¼ é€’å®æ—¶æ¶ˆæ¯
# æ•°æ®ç»“æ„: {client_id: deque([message1, message2, ...])}
message_queue = defaultdict(deque)
# å®¢æˆ·ç«¯æœ€åæ´»åŠ¨æ—¶é—´ - ç”¨äºæ¸…ç†éæ´»è·ƒè¿æ¥
# æ•°æ®ç»“æ„: {client_id: timestamp}
client_last_seen = {}
# ä»»åŠ¡çŠ¶æ€è·Ÿè¸ª - è®°å½•å·¥ä½œæµæ‰§è¡Œè¿›åº¦
# æ•°æ®ç»“æ„: {prompt_id: {type, data, timestamp, enhanced}}
task_status = {}
# çº¿ç¨‹é” - ç¡®ä¿æ¶ˆæ¯é˜Ÿåˆ—æ“ä½œçš„çº¿ç¨‹å®‰å…¨
broadcast_lock = threading.Lock()
queue_lock = threading.Lock()

def start_progress_tracker_by_mapping(prompt_id, workflow_id, client_id, comfyui_url):
    """
    åˆå§‹åŒ–å·¥ä½œæµè¿›åº¦è·Ÿè¸ªå™¨
    æ ¹æ®å·¥ä½œæµæ˜ å°„é…ç½®å¯åŠ¨è¿›åº¦è·Ÿè¸ªï¼Œç›‘æ§ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€å¹¶å‘å®¢æˆ·ç«¯æ¨é€æ›´æ–°
    
    å‚æ•°:
        prompt_id (str): ä»»åŠ¡IDï¼Œç”¨äºæ ‡è¯†ç‰¹å®šå·¥ä½œæµæ‰§è¡Œå®ä¾‹
        workflow_id (str): å·¥ä½œæµæ¨¡æ¿ID
        client_id (str): å®¢æˆ·ç«¯IDï¼Œç”¨äºå®šå‘æ¨é€è¿›åº¦æ¶ˆæ¯
        comfyui_url (str): ComfyUIæœåŠ¡åœ°å€
    """
    try:
        workflow_mappings = proxy.mappings.get('workflow_mappings', {})
        workflow_info = workflow_mappings.get(workflow_id, {})
        node_mappings = workflow_info.get('node_mappings', {})
        total_nodes = len(node_mappings)

        proxy.workflow_node_count[workflow_id] = total_nodes

        task_status[prompt_id] = {
            "type": "start",
            "data": {
                "prompt_id": prompt_id,
                "workflow_id": workflow_id,
                "client_id": client_id,
                "total_nodes": total_nodes,
                "completed_nodes": 0,
                "node_status": {}
            },
            "timestamp": time.time()
        }

        logger.info(f"ğŸ“Š å·²åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª: {workflow_id} (èŠ‚ç‚¹æ€»æ•°: {total_nodes})")
    except Exception as e:
        logger.error(f"âŒ è¿›åº¦è·Ÿè¸ªåˆå§‹åŒ–å¤±è´¥: {e}")


def add_message_to_queue(client_id, message):
    """
    å‘æŒ‡å®šå®¢æˆ·ç«¯çš„æ¶ˆæ¯é˜Ÿåˆ—æ·»åŠ æ¶ˆæ¯ï¼Œå¹¶æ·»åŠ æ—¶é—´æˆ³å’Œç±»å‹ä¿¡æ¯
    çº¿ç¨‹å®‰å…¨è®¾è®¡ï¼Œä½¿ç”¨queue_lockç¡®ä¿å¤šçº¿ç¨‹ç¯å¢ƒä¸‹çš„æ•°æ®ä¸€è‡´æ€§
    
    å‚æ•°:
        client_id (str): å®¢æˆ·ç«¯å”¯ä¸€æ ‡è¯†ç¬¦
        message (dict): è¦å‘é€çš„æ¶ˆæ¯å­—å…¸ï¼Œå¿…é¡»åŒ…å«'type'å­—æ®µ
    """
    with queue_lock:
        if client_id not in message_queue:
            message_queue[client_id] = deque()
        
        # å¢å¼ºæ¶ˆæ¯å†…å®¹ï¼Œæ·»åŠ æ—¶é—´æˆ³
        enhanced_message = {
            **message,
            "timestamp": time.time()
        }
        message_queue[client_id].append(enhanced_message)
        logger.info(f"ğŸ“¨ æ¶ˆæ¯å·²æ·»åŠ åˆ°å®¢æˆ·ç«¯é˜Ÿåˆ—: {client_id} (ç±»å‹: {message.get('type', 'unknown')})")

def get_messages_for_client(client_id, since_timestamp=None):
    """
    è·å–å®¢æˆ·ç«¯è‡ªæŒ‡å®šæ—¶é—´æˆ³ä»¥æ¥çš„æœªè¯»æ¶ˆæ¯ï¼Œå¹¶æ¸…ç†å†å²æ¶ˆæ¯
    ä¿ç•™æœ€è¿‘20æ¡æ¶ˆæ¯ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨
    
    å‚æ•°:
        client_id (str): å®¢æˆ·ç«¯å”¯ä¸€æ ‡è¯†ç¬¦
        since_timestamp (float): æ—¶é—´æˆ³ï¼Œä»…è¿”å›æ­¤æ—¶é—´ä¹‹åçš„æ¶ˆæ¯
    è¿”å›:
        list: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¯æ¡æ¶ˆæ¯åŒ…å«'timestamp'å’ŒåŸå§‹å†…å®¹
    """
    
    with queue_lock:
        client_last_seen[client_id] = time.time()
        
        if client_id not in message_queue:
            return []
        
        messages = []
        for msg in message_queue[client_id]:
            if since_timestamp is None or msg["timestamp"] > since_timestamp:
                messages.append(msg)
        
        # æ¸…ç†å·²è¯»æ¶ˆæ¯ï¼ˆä¿ç•™æœ€è¿‘20æ¡ï¼‰
        if len(messages) > 0:
            message_queue[client_id] = deque(list(message_queue[client_id])[-20:])
        
        return messages

def broadcast_message(message):
   
    current_time = time.time()
    active_clients = []
    
    with queue_lock:
        # æ‰¾å‡ºæ´»è·ƒå®¢æˆ·ç«¯ï¼ˆæœ€è¿‘5åˆ†é’Ÿå†…æœ‰æ´»åŠ¨ï¼‰
        for client_id, last_seen in client_last_seen.items():
            if current_time - last_seen < 300:  # 5åˆ†é’Ÿ
                active_clients.append(client_id)
    

    for client_id in active_clients:
        add_message_to_queue(client_id, message)

def cleanup_inactive_clients():

    current_time = time.time()
    inactive_clients = []
    
    with queue_lock:
        for client_id, last_seen in client_last_seen.items():
            if current_time - last_seen > 600:  # 10åˆ†é’Ÿæ— æ´»åŠ¨
                inactive_clients.append(client_id)
        
        for client_id in inactive_clients:
            del client_last_seen[client_id]
            if client_id in message_queue:
                del message_queue[client_id]
            if client_id in upload_progress:
                del upload_progress[client_id]
    
    if inactive_clients:
        logger.info(f"ğŸ§¹ æ¸…ç†äº† {len(inactive_clients)} ä¸ªéæ´»è·ƒå®¢æˆ·ç«¯")


def comfy_ws_listener():
    """
    ComfyUI WebSocketç›‘å¬å™¨
    å»ºç«‹ä¸ComfyUIçš„WebSocketè¿æ¥ï¼Œæ¥æ”¶å®æ—¶æ‰§è¡ŒçŠ¶æ€æ¶ˆæ¯å¹¶å¢å¼ºå¤„ç†åè½¬å‘ç»™å®¢æˆ·ç«¯
    æ”¯æŒçš„æ¶ˆæ¯ç±»å‹: status, executing, progress, executed
    
    æ¶ˆæ¯å¢å¼º: æ·»åŠ è¿›åº¦ç™¾åˆ†æ¯”ã€èŠ‚ç‚¹IDã€æ—¶é—´æˆ³ç­‰é¢å¤–ä¿¡æ¯ï¼Œä¾¿äºå‰ç«¯å±•ç¤º
    """
    import websocket
    import json
    import copy

    def enhance_message(original_message):
        """å¢å¼ºåŸå§‹æ¶ˆæ¯ï¼Œæ·»åŠ é¢å¤–å…ƒæ•°æ®"""
        enhanced = copy.deepcopy(original_message)
        msg_type = enhanced.get("type")
        data = enhanced.get("data", {})

        if msg_type == "status":
            status = data.get("status", {})
            exec_info = status.get("exec_info", {})
            enhanced["data"]["enhanced_info"] = {
                "queue_remaining": exec_info.get("queue_remaining", 0),
                "queue_running": len(exec_info.get("queue_running", [])),
                "timestamp": time.time()
            }

        elif msg_type == "executing":
            node_id = data.get("node")
            prompt_id = data.get("prompt_id")
            enhanced["data"]["enhanced_info"] = {
                "node_id": node_id,
                "prompt_id": prompt_id,
                "status": "executing",
                "timestamp": time.time()
            }

        elif msg_type == "progress":
            value = data.get("value", 0)
            max_value = data.get("max", 0)
            node_id = data.get("node")
            prompt_id = data.get("prompt_id")
            try:
                percentage = round((value / max_value) * 100, 1) if max_value else 0
            except:
                percentage = 0
            enhanced["data"]["enhanced_info"] = {
                "percentage": percentage,
                "current_step": value,
                "total_steps": max_value,
                "node_id": node_id or "unknown",
                "prompt_id": prompt_id or "unknown",
                "is_sampling": str(data.get("name", "")).lower().startswith("ksampler"),
                "timestamp": time.time()
            }

        elif msg_type == "executed":
            prompt_id = data.get("prompt_id")
            node_id = data.get("node")
            enhanced["data"]["enhanced_info"] = {
                "prompt_id": prompt_id,
                "node_id": node_id,
                "status": "completed",
                "timestamp": time.time()
            }

        return enhanced

    def on_message(ws, message):
        try:
            msg_json = json.loads(message)
            enhanced = enhance_message(msg_json)

            msg_type = msg_json.get("type")
            data = msg_json.get("data", {})

            if msg_type == "progress":
                prompt_id = data.get("prompt_id")
                value = data.get("value", 0)
                max_value = data.get("max", 1)
                percent = int((value / max_value) * 100) if max_value else 0
                node_id = data.get("node")

                if prompt_id in task_status:
                    task_status[prompt_id]["type"] = "progress"
                    task_status[prompt_id]["data"].update({
                        "percentage": percent,
                        "current_step": value,
                        "total_steps": max_value,
                        "node_id": node_id,
                        "is_sampling": str(data.get("name", "")).lower().startswith("ksampler"),
                    })
                    task_status[prompt_id]["timestamp"] = time.time()
                    logger.info(f"ğŸ“ˆ è¿›åº¦æ›´æ–°: {percent}% [{value}/{max_value}] @èŠ‚ç‚¹ {node_id}")

            elif msg_type == "executing":
                prompt_id = data.get("prompt_id")
                node_id = data.get("node")

                if prompt_id in task_status:
                    task_status[prompt_id]["type"] = "executing"
                    task_status[prompt_id]["data"].update({
                        "node_id": node_id,
                        "status": "executing"
                    })
                    task_status[prompt_id]["timestamp"] = time.time()
                    logger.info(f"âš™ï¸ [æ‰§è¡Œä¸­] {prompt_id} èŠ‚ç‚¹: {node_id}")

            elif msg_type == "executed":
                prompt_id = data.get("prompt_id")

                if prompt_id in task_status:
                    task_status[prompt_id]["type"] = "done"
                    task_status[prompt_id]["data"].update({
                        "status": "done"
                    })
                    task_status[prompt_id]["timestamp"] = time.time()
                    logger.info(f"âœ… [å®Œæˆ] {prompt_id}")

        
            client_id = task_status.get(data.get("prompt_id"), {}).get("data", {}).get("client_id")
            if client_id:
                add_message_to_queue(client_id, enhanced)

        except Exception as e:
            logger.warning(f"âš ï¸ WebSocketæ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
    logging.getLogger("websocket").setLevel(logging.CRITICAL)
    def on_error(ws, error):
        logger.error(f"è¿œç¨‹ç›‘å¬æœåŠ¡å°šæœªå¼€å¯ï¼Œè¯·ç­‰å¾…...")
    def on_close(ws, close_status_code, close_msg):
        logger.warning(f"ç­‰å¾…æ ¸å¿ƒæœåŠ¡ç¨‹åºå¯åŠ¨ï¼Œå¼€å§‹å°è¯•å»ºç«‹è¿æ¥")
        
    def on_open(ws):
        logger.info("ğŸ”— [ComfyUI WS] è¿æ¥å·²å»ºç«‹")
def start_ws_listener(base_url):
    if not base_url or not base_url.startswith("http"):
        print("âš ï¸ è·³è¿‡ WebSocket åˆå§‹åŒ–ï¼šæ— æœ‰æ•ˆ URL")
        return

    import websocket

    ws_url = base_url.replace("http://", "ws://").replace("https://", "wss://") + "/ws"
    ws = websocket.WebSocketApp(
        ws_url,
        on_message=on_message,
        on_error=on_error,
        on_close=on_close,
        on_open=on_open
    )

    thread = Thread(target=ws.run_forever, daemon=True)
    thread.start()


def cleanup_task():
   
    while True:
        try:
            cleanup_inactive_clients()
            
            #  æ¸…ç†è¿‡æœŸä»»åŠ¡
            current_time = time.time()
            expired_tasks = []
            for prompt_id, status_info in task_status.items():
                if current_time - status_info["timestamp"] > 7200:  # 2å°æ—¶
                    expired_tasks.append(prompt_id)
            
            for prompt_id in expired_tasks:
                del task_status[prompt_id]
            
            if expired_tasks:
                logger.info(f"ğŸ§¹ æ¸…ç†äº† {len(expired_tasks)} ä¸ªè¿‡æœŸä»»åŠ¡çŠ¶æ€")
                
        except Exception as e:
            logger.error(f"æ¸…ç†ä»»åŠ¡å¼‚å¸¸: {e}")
        
        time.sleep(60)  # æ¯60ç§’æ‰§è¡Œä¸€æ¬¡

 
class HuiYingProxy:
    """
    ç»˜å½±ä»£ç†æ ¸å¿ƒç±»ï¼Œè´Ÿè´£å·¥ä½œæµç®¡ç†ã€å‚æ•°æ˜ å°„å’ŒComfyUIé€šä¿¡
    
    è¯¥ç±»åè°ƒé…ç½®åŠ è½½ã€å·¥ä½œæµå¤„ç†å’Œä»»åŠ¡æäº¤çš„å…¨è¿‡ç¨‹ï¼Œæ”¯æŒæœ¬åœ°å’Œäº‘ç«¯ä¸¤ç§æ¨¡å¼ï¼š
    - æœ¬åœ°æ¨¡å¼ï¼šä»æœ¬åœ°æ–‡ä»¶ç³»ç»ŸåŠ è½½é…ç½®å’Œå·¥ä½œæµ
    - äº‘ç«¯æ¨¡å¼ï¼šä»è¿œç¨‹æœåŠ¡å™¨è·å–é…ç½®å’Œå·¥ä½œæµèµ„æº
    """
    def __init__(self, config_file='config.json', mappings_file='workflow_mappings.json'):
        """
        åˆå§‹åŒ–HuiYingProxyå®ä¾‹
        
        å‚æ•°:
            config_file (str): é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º'config.json'
            mappings_file (str): å·¥ä½œæµå‚æ•°æ˜ å°„æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º'workflow_mappings.json'
        """
        self.config_file = config_file
        self.mappings_file = mappings_file
        self.config = self.load_config(config_file)
        self.mappings = self.load_mappings(mappings_file)
        self.workflow_cache = {}  # å·¥ä½œæµç¼“å­˜: {workflow_id: workflow_data}
        self.workflow_node_count = {}  # å·¥ä½œæµèŠ‚ç‚¹è®¡æ•°: {workflow_id: node_count}
        if self.config.get('load_from_cloud'):
            self.preload_workflows()

    def save_config(self):
        """
        å°†å½“å‰é…ç½®ä¿å­˜åˆ°ç£ç›˜
        ç¡®ä¿é…ç½®æ›´æ”¹æŒä¹…åŒ–ï¼Œç”¨äºä¿å­˜ç”¨æˆ·è®¾ç½®å’Œç³»ç»Ÿé…ç½®
        """
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=2, ensure_ascii=False)
            logger.info("ğŸ’¾ é…ç½®å·²ä¿å­˜")
        except Exception as e:
            logger.warning(f"âš ï¸ é…ç½®ä¿å­˜å¤±è´¥: {e}")

    def load_config(self, config_file):
        """
        åŠ è½½å¹¶åˆå¹¶é…ç½®æ–‡ä»¶
        ä»æŒ‡å®šæ–‡ä»¶åŠ è½½ç”¨æˆ·é…ç½®ï¼Œä¸é»˜è®¤é…ç½®åˆå¹¶ï¼Œå¤„ç†è·¯å¾„æ ‡å‡†åŒ–
        
        å‚æ•°:
            config_file (str): é…ç½®æ–‡ä»¶è·¯å¾„
        è¿”å›:
            dict: åˆå¹¶åçš„å®Œæ•´é…ç½®å­—å…¸
        """
        
        default_config = {
            "workflow_dir": "workflows",
            "comfyui_url": "",
            "cloud_service_url": "proxy.hueying.cn",
            "proxy_port": 8080,
            "load_from_cloud": False,
            "timeout": 30,
            "enable_parameter_validation": True,
            "enable_workflow_cache": True,
            "log_level": "INFO"
        }
        logger.info(f"ğŸ“ å¼€å§‹æ‰«ææ‰€éœ€çš„å¿…è¦æ–‡ä»¶")

        if os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                default_config.update(user_config)
                logger.info(f"âœ… åŸºç¡€é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_file}")
    
            except Exception as e:
                logger.error(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        else:
        
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ†• åˆå§‹åŒ–é»˜è®¤é…ç½®æ–‡ä»¶æˆåŠŸ: {config_file}")
            logger.info(f"ğŸ’¾ å·²ä¿å­˜ ComfyUI åœ°å€é…ç½®: {default_config['comfyui_url']}")
  
        default_config["workflow_dir"] = os.path.abspath(default_config["workflow_dir"])
        default_config["comfyui_url"] = sanitize_url(default_config["comfyui_url"])
        if default_config.get('load_from_cloud'):
            logger.info("ğŸ“ å°†ä»äº‘ç«¯åŠ è½½å·¥ä½œæµå’Œæ˜ å°„é…ç½®")
        else:
            logger.info(f"ğŸ“ å½“å‰å·¥ä½œæµè·¯å¾„ä¸º: {default_config['workflow_dir']}")
        logger.info(f"ğŸ”— å½“å‰ ComfyUI åœ°å€: {default_config['comfyui_url']}")


        return default_config

    def load_mappings(self, mappings_file):
        if self.config.get('load_from_cloud'):
            return self.fetch_remote_mappings()

        if not os.path.exists(mappings_file):
            logger.warning(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {mappings_file}")
            return {}
        
        try:
            with open(mappings_file, 'r', encoding='utf-8') as f:
                try:
                    mappings = json.load(f)
                    logger.info(f"âœ… åˆå§‹åŒ–å‚æ•°æ˜ å°„é…ç½®æˆåŠŸ: {mappings_file}")
                    return mappings
                except json.JSONDecodeError as e:
                    logger.error(f"ğŸ”¥ JSONè¯­æ³•é”™è¯¯: {e.msg}ï¼Œä½ç½®ï¼šè¡Œ{e.lineno}åˆ—{e.colno}")
                    with open(mappings_file, 'r', encoding='utf-8') as f_context:
                        lines = f_context.readlines()
                        context = lines[max(0, e.lineno-2):e.lineno+1]
                        logger.error(f"ğŸ”¥ é”™è¯¯è¡Œä¸Šä¸‹æ–‡:\n{''.join(context)}")
                    return {}
        except Exception as e:
            logger.error(f"ğŸ”¥ é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
            return {}

    def load_workflow(self, workflow_id):


        if self.config.get('load_from_cloud'):
            if workflow_id in self.workflow_cache:
                logger.info(f"âš¡ ä»ç¼“å­˜ä¸­åŠ è½½äº‘ç«¯å·¥ä½œæµ: {workflow_id}")
                return self.workflow_cache[workflow_id]
            return self.fetch_remote_workflow(workflow_id)

        if self.config.get('enable_workflow_cache', True) and workflow_id in self.workflow_cache:
            logger.info(f"âš¡ ä»ç¼“å­˜ä¸­åŠ è½½å½“å‰å·¥ä½œæµ: {workflow_id}")
            return self.workflow_cache[workflow_id]
            
        workflow_file = os.path.join(self.config['workflow_dir'], f"{workflow_id}.json")
        
        if not os.path.exists(workflow_file):
            logger.error(f"âŒ å·¥ä½œæµæ–‡ä»¶ä¸å­˜åœ¨: {workflow_file}")
            raise FileNotFoundError(f"å·¥ä½œæµæ–‡ä»¶ä¸å­˜åœ¨: {workflow_file}")
            
        try:
            with open(workflow_file, 'r', encoding='utf-8') as f:
                workflow = json.load(f)
     
            if self.config.get('enable_workflow_cache', True):
                self.workflow_cache[workflow_id] = workflow
            
            logger.info(f"ğŸ“„ ä»ç¼“å­˜ä¸­è¯»å–åˆ°å½“å‰å·¥ä½œæµ: {workflow_id}")
            return workflow
            
        except Exception as e:
            logger.error(f"å·¥ä½œæµåŠ è½½å¤±è´¥ {workflow_id}: {e}")
            raise
      
    def _set_nested_value(self, data, path, value):

        try:
            for key in path[:-1]:
                if isinstance(key, int):
                    data = data[key]
                else:
                    data = data.setdefault(key, {})
            last_key = path[-1]
            if isinstance(last_key, int):
                data[last_key] = value
            else:
                data[last_key] = value
            logger.debug(f"âœ… è®¾ç½®è·¯å¾„ {path} = {value}")
        except Exception as e:
            logger.error(f"âŒ è®¾ç½®å‚æ•°å¤±è´¥: path={path}, value={value}, é”™è¯¯: {e}")

    def fetch_remote_mappings(self):
        cloud_url = sanitize_url(self.config.get('cloud_service_url', ''))
        url = f"{cloud_url}/workflow_mappings.json"
        try:
            resp = requests.get(url, timeout=self.config.get('timeout', 30))
            resp.raise_for_status()
            logger.info(f"âœ… ä»äº‘ç«¯åŠ è½½æ˜ å°„é…ç½®æˆåŠŸ: {url}")
            return resp.json()
        except Exception as e:
            logger.error(f"âŒ åŠ è½½äº‘ç«¯æ˜ å°„é…ç½®å¤±è´¥: {e}")
            return {}

    def fetch_remote_workflow(self, workflow_id):
        cloud_url = sanitize_url(self.config.get('cloud_service_url', ''))
        url = f"{cloud_url}/workflows/{workflow_id}.json"
        try:
            resp = requests.get(url, timeout=self.config.get('timeout', 30))
            resp.raise_for_status()
            workflow = resp.json()
            self.workflow_cache[workflow_id] = workflow
            logger.info(f"âœ… ä»äº‘ç«¯åŠ è½½å·¥ä½œæµ: {workflow_id}")
            return workflow
        except Exception as e:
            logger.error(f"âŒ è·å–äº‘ç«¯å·¥ä½œæµ {workflow_id} å¤±è´¥: {e}")
            return {}

    def preload_workflows(self):
        workflow_ids = self.mappings.get('workflow_mappings', {}).keys()
        for wid in workflow_ids:
            self.fetch_remote_workflow(wid)

    def merge_workflow_params(self, workflow, param_dict, workflow_id):
      
        try:
            merged_workflow = copy.deepcopy(workflow)

            workflow_mappings = self.mappings.get('workflow_mappings', {})
            param_mappings = workflow_mappings.get(workflow_id, {}).get('param_mappings', {})

            logger.info(f"ğŸ”§ åŒ¹é…åˆ°ç»˜å½± AIGC å‘é€çš„ {len(param_dict)} ä¸ªå‚æ•°")

            for param_key, param_value in param_dict.items():
                if isinstance(param_value, str) and param_value.startswith("é»˜è®¤"):
                    #logger.info(f"ğŸ†— é»˜è®¤å‚æ•°: {param_key} = {param_value}")
                    continue
                if param_key not in param_mappings:
                    logger.debug(f"â­ï¸ æœªæ˜ å°„å‚æ•°: {param_key}")
                    continue

                path = param_mappings[param_key]
                self._set_nested_value(merged_workflow, path, param_value)

            return merged_workflow

        except Exception as e:
            logger.error(f"âŒ å‚æ•°åˆå¹¶å¤±è´¥: {e}")
            raise
       
    def send_to_comfyui(self, workflow_data, client_id, comfyui_url=None):

        import requests

        if not comfyui_url:
            comfyui_url = self.config.get("comfyui_url")
        if not comfyui_url:
            raise ValueError("comfyui_url is required")
        comfyui_url = sanitize_url(comfyui_url)
        workflow_data = adapt_workflow_paths(workflow_data, comfyui_url)
        url = f"{comfyui_url}/prompt"

        try:
            headers = {
                "Content-Type": "application/json"
            }
            payload = {
                "client_id": client_id,
                "prompt": workflow_data
            }
            logger.info(f"ğŸš€ æ­£åœ¨æäº¤ä»»åŠ¡åˆ° ç”ŸæˆæœåŠ¡å™¨: {url}")
            response = requests.post(url, json=payload, headers=headers, timeout=self.config.get("timeout", 30))

            if response.status_code == 200:
                logger.info("âœ… ä»»åŠ¡æäº¤æˆåŠŸ")
                return {"data": response.json()}
            else:
                logger.error(f"âŒ ä»»åŠ¡è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, å†…å®¹: {response.text}")
                return {"error": f"ä»»åŠ¡ è¯·æ±‚å¤±è´¥: {response.status_code}", "detail": response.text}

        except Exception as e:
            logger.error(f"âŒ ComfyUIè¯·æ±‚å¤±è´¥: {str(e)}")
            return {"error": str(e)}
proxy = HuiYingProxy()
COMFYUI_URL = sanitize_url(proxy.config.get("comfyui_url", ""))


@app.before_request
def log_all_requests():
    logger.info(f"ğŸ“¡ æ”¶åˆ°æ’ä»¶æ¥å£è¯·æ±‚: {request.method} {request.path}")

# å¤„ç†è·¨åŸŸè¯·æ±‚
@app.route('/api/poll', methods=['GET'])
def poll_messages():
    """
    å®¢æˆ·ç«¯æ¶ˆæ¯è½®è¯¢æ¥å£
    ä¾›å‰ç«¯å®šæœŸè°ƒç”¨ä»¥è·å–æ–°æ¶ˆæ¯ï¼Œæ”¯æŒå¢é‡è·å–ï¼ˆåªè¿”å›æŒ‡å®šæ—¶é—´æˆ³åçš„æ¶ˆæ¯ï¼‰
    åŒæ—¶æ›´æ–°å®¢æˆ·ç«¯æœ€åæ´»åŠ¨æ—¶é—´ï¼Œç”¨äºè¿æ¥çŠ¶æ€ç®¡ç†
    
    è¯·æ±‚å‚æ•°:
        clientId (str): å®¢æˆ·ç«¯å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œå¿…éœ€
        since (float): å¯é€‰ï¼Œæ—¶é—´æˆ³ï¼Œåªè¿”å›æ­¤æ—¶é—´ä¹‹åçš„æ¶ˆæ¯
    è¿”å›:
        json: åŒ…å«æ¶ˆæ¯åˆ—è¡¨å’ŒæœåŠ¡å™¨çŠ¶æ€çš„å“åº”
    """
    client_id = request.args.get('clientId')
    since_timestamp = request.args.get('since', type=float)
    
    if not client_id:
        return jsonify({"error": "ç¼ºå°‘clientIdå‚æ•°"}), 400
    
    try:
        messages = get_messages_for_client(client_id, since_timestamp)
        
        
        extra_info = {
            "active_tasks": len(task_status),
            "queue_size": len(message_queue.get(client_id, [])),
            "server_time": time.time()
        }
        
        return jsonify({
            "code": 0,
            "msg": "success",
            "data": {
                "messages": messages,
                "timestamp": time.time(),
                "clientId": client_id,
                "extra_info": extra_info
            }
        })
        
    except Exception as e:
        logger.error(f"è½®è¯¢æ¶ˆæ¯å¤±è´¥: {e}")
        return jsonify({"error": f"è½®è¯¢å¤±è´¥: {str(e)}"}), 500

@app.route('/api/task_status/<prompt_id>', methods=['GET'])
def get_task_status(prompt_id):
    """
    è·å–ä»»åŠ¡çŠ¶æ€æ¥å£
    æŸ¥è¯¢æŒ‡å®šprompt_idçš„å·¥ä½œæµæ‰§è¡ŒçŠ¶æ€ï¼ŒåŒ…æ‹¬è¿›åº¦ã€èŠ‚ç‚¹ä¿¡æ¯å’Œæ—¶æ•ˆæ€§
    
    è¯·æ±‚å‚æ•°:
        prompt_id (str): ä»»åŠ¡IDï¼Œä»URLè·¯å¾„è·å–
    è¿”å›:
        json: åŒ…å«ä»»åŠ¡çŠ¶æ€ã€æ‰§è¡Œä¿¡æ¯å’Œæ—¶æ•ˆæ€§çš„å“åº”
    """
    try:
        if prompt_id in task_status:
            status_info = task_status[prompt_id]
            
            enhanced_status = {
                **status_info,
                "age_seconds": time.time() - status_info["timestamp"],
                "is_recent": (time.time() - status_info["timestamp"]) < 300  # 5åˆ†é’Ÿå†…
            }
            
            return jsonify({
                "code": 0,
                "msg": "success",
                "data": enhanced_status
            })
        else:
            return jsonify({
                "code": 404,
                "msg": "ä»»åŠ¡çŠ¶æ€ä¸å­˜åœ¨",
                "data": None
            }), 404
            
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({"error": f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}"}), 500
# #comfyuiå¯¹è±¡ä¿¡æ¯æ¥å£
# @app.route('/api/object_info', methods=['GET'])
# def proxy_object_info():
  
#     try:
#         comfyui_url = request.args.get('comfyuiUrl') or proxy.config.get('local_comfyui_url', COMFYUI_URL)
#         comfyui_url = sanitize_url(comfyui_url)
#         res = requests.get(f"{comfyui_url}/object_info", timeout=10)
#         logger.info("ğŸ” æˆåŠŸè½¬å‘ object_infoï¼ŒçŠ¶æ€ç : %s", res.status_code)
#         return Response(
#             res.content,
#             status=res.status_code,
#             content_type=res.headers.get('Content-Type', 'application/json')
#         )
#     except Exception as e:
#         logger.error(f"âŒ object_info è½¬å‘å¤±è´¥: {e}")
#         return jsonify({"error": f"è¿æ¥å¤±è´¥: {str(e)}"}), 500
# #å›¾åƒä¸Šä¼ æ¥å£
# @app.route('/upload/image', methods=['POST'])
# def proxy_upload_image():
#     logger.info("ğŸ–¼ï¸ æ”¶åˆ°æ’ä»¶ä¸Šä¼ å›¾åƒè¯·æ±‚ï¼Œå¼€å§‹è½¬å‘ç»™çœŸå® ComfyUI")
#     try:
#         data = request.form.to_dict()
#         files = {
#             key: (f.filename, f.stream, f.mimetype)
#             for key, f in request.files.items()
#         }
#         comfyui_url = data.get('comfyuiUrl') or proxy.config.get('local_comfyui_url', COMFYUI_URL)
#         comfyui_url = sanitize_url(comfyui_url)
#         resp = requests.post(f"{comfyui_url}/upload/image", data=data, files=files)
#         logger.info(f"ğŸ“¤ æˆåŠŸè½¬å‘å›¾åƒä¸Šä¼ è¯·æ±‚ï¼ŒçŠ¶æ€ç : {resp.status_code}")
#         return (resp.content, resp.status_code, resp.headers.items())
#     except Exception as e:
#         logger.exception("âŒ è½¬å‘å›¾åƒä¸Šä¼ å¤±è´¥:")
#         return jsonify({"code": 500, "msg": "å›¾åƒä¸Šä¼ è½¬å‘å¤±è´¥", "error": str(e)}), 500

# #è’™ç‰ˆæ¥å£ï¼ˆé¢„ç•™ï¼‰
# @app.route('/api/upload/mask', methods=['POST'])
# def proxy_upload_mask():
#     logger.info("ğŸ–¤ æ”¶åˆ°æ’ä»¶ä¸Šä¼  mask è¯·æ±‚ï¼Œå¼€å§‹è½¬å‘ç»™çœŸå® ComfyUI")
#     try:
#         data = request.form.to_dict()
#         files = {
#             key: (f.filename, f.stream, f.mimetype)
#             for key, f in request.files.items()
#         }
#         comfyui_url = data.get('comfyuiUrl') or proxy.config.get('local_comfyui_url', COMFYUI_URL)
#         comfyui_url = sanitize_url(comfyui_url)
#         resp = requests.post(f"{comfyui_url}/upload/mask", data=data, files=files)
#         logger.info(f"ğŸ“¤ æˆåŠŸè½¬å‘ mask ä¸Šä¼ è¯·æ±‚ï¼ŒçŠ¶æ€ç : {resp.status_code}")
#         return (resp.content, resp.status_code, resp.headers.items())
#     except Exception as e:
#         logger.exception("âŒ mask è½¬å‘å¤±è´¥:")
#         return jsonify({"code": 500, "msg": "ä¸Šä¼  mask è½¬å‘å¤±è´¥", "error": str(e)}), 500
#æ•°æ®æäº¤æ¥å£
@app.route('/psPlus/workflow/huiYingCommit', methods=['POST'])
def huiying_commit():

    # æ ¡éªŒ token æ˜¯å¦æœ‰æ•ˆ
    auth_header = request.headers.get("Authorization", "")
    token = auth_header.replace("Bearer ", "")
    username = sessions.get(token)
    if not username:
        logger.warning("âŒ æäº¤è¢«æ‹’ç»ï¼Œç”¨æˆ·æœªç™»å½•æˆ– token å·²å¤±æ•ˆ")
        return jsonify({"code": 401, "msg": "æœªæˆæƒè®¿é—®ï¼Œè¯·é‡æ–°ç™»å½•"}), 401

    data = request.get_json() or {}

    # logger.debug(f"ğŸŒ æ”¶åˆ°å®Œæ•´æ•°æ®: {json.dumps(data, indent=2, ensure_ascii=False)}")
    # logger.info("ğŸ› ï¸ æ­£åœ¨å¤„ç†ç»˜å½±å·¥ä½œæµæäº¤è¯·æ±‚...")

    # try:
    #     #logger.debug(f"ğŸ” Headers: {dict(request.headers)}")

    #     if request.is_json:
    #         #logger.debug(f"ğŸ§¾ JSON Body: {request.get_json()}")
    #     else:
    #         #logger.debug(f"ğŸ§¾ é JSON è¯·æ±‚ä½“")
    # except Exception as e:
    #     #logger.error(f"âŒ æ‰“å°è¯·æ±‚å¤±è´¥: {e}")

    try:
      
        logger.info("ğŸ“ æ¥æ”¶æ¥è‡ª ç»˜å½± AI çš„æ•°æ®å­—æ®µ")
        if not data:
            return jsonify({"code": 400, "msg": "è¯·æ±‚æ•°æ®ä¸ºç©º"}), 400
        
        workflow_id = data.get('workflowId')
        param_dict = data.get('paramDict', {})
        client_id = data.get('clientId', str(uuid.uuid4()))
        comfyui_url = data.get('comfyuiUrl') or proxy.config.get('comfyui_url')
        if not comfyui_url:
            return jsonify({"code": 400, "msg": "missing comfyuiUrl"}), 400
        comfyui_url = sanitize_url(comfyui_url)
        
       
        if not workflow_id:
            return jsonify({"code": 400, "msg": "workflowIdä¸èƒ½ä¸ºç©º"}), 400
        
        
        try:
            workflow = proxy.load_workflow(workflow_id)
            logger.info(f"ğŸ“¦ å·¥ä½œæµåŠ è½½æˆåŠŸ: {workflow_id}")
            logger.info(f"ğŸ“Š å­˜åœ¨æ€»èŠ‚ç‚¹æ•°: {len(workflow)}")
            logger.info(f"ğŸ“¥ æ¥æ”¶å‚æ•°æ•°é‡: {len(param_dict)}")
            
           
            for k, v in param_dict.items():
                logger.debug(f"  â”œâ”€ å‚æ•°: {k} = {v}")
                
        except FileNotFoundError:
            return jsonify({"code": 404, "msg": f"å·¥ä½œæµä¸å­˜åœ¨: {workflow_id}"}), 404
        except Exception as e:
            return jsonify({"code": 500, "msg": f"å·¥ä½œæµåŠ è½½å¤±è´¥: {str(e)}"}), 500
        
      
        try:
            merged_workflow = proxy.merge_workflow_params(workflow, param_dict, workflow_id)

            
            total_nodes = len([
                k for k, v in merged_workflow.items()
                if isinstance(v, dict) and "class_type" in v
            ])

            logger.info(f"ğŸ“Š å‚æ•°åˆå¹¶æ ¡éªŒå®Œæ¯• ")

          
            valid_workflow = {}
            for key, value in merged_workflow.items():
                if isinstance(value, dict) and "class_type" in value:
                    valid_workflow[key] = value
                else:
                    logger.warning(f"âš ï¸ ç§»é™¤éæ³•èŠ‚ç‚¹: {key}")
            merged_workflow = valid_workflow

            try:
                BASE_DIR = os.path.dirname(os.path.abspath(__file__))
                model_name = None
                sampler_name = "N/A"
                scheduler = "N/A"
                steps = "N/A"
                cfg = "N/A"
                denoise = "N/A"
                seed = "N/A"

                has_checkpoint = False

                for node_id, node in merged_workflow.items():
                    if not isinstance(node, dict):
                        continue
                    inputs = node.get("inputs", {})
                    class_type = node.get("class_type", "")

                    if class_type == "CheckpointLoaderSimple":
                        model_name = inputs.get("ckpt_name")
                        has_checkpoint = True
                    elif class_type == "UNetLoader" and not has_checkpoint:
                        model_name = "UNET åŠå…¶ä»–"

                    if class_type in ["KSampler", "KSamplerAdvanced"]:
                        sampler_name = inputs.get("sampler_name", sampler_name)
                        scheduler = inputs.get("scheduler", scheduler)
                        steps = inputs.get("steps", steps)
                        cfg = inputs.get("cfg", cfg)
                        denoise = inputs.get("denoise", denoise)
                        seed = inputs.get("seed", seed)

                    if "image" in inputs:
                        image_path = inputs["image"]
                        width = inputs.get("width", "æœªçŸ¥")
                        height = inputs.get("height", "æœªçŸ¥")
                        image_size = f"{width}x{height}"


                seed_info = f"{seed}ï¼ˆéšæœºï¼‰" if str(seed) in ["-1", "None", "-1.0"] else str(seed)
                model_display = model_name if model_name else "UNET åŠå…¶ä»–"

                logger.info("ğŸ“¤ ******** å·¥ä½œæµæ¦‚è¦ ********")
                logger.info(f"ğŸ¯ å·¥ä½œæµ ID: {workflow_id}")
                logger.info(f"ğŸ¤– æ¨¡å‹: {model_display}")
                logger.info(f"âš™ï¸ é‡‡æ ·å™¨: {sampler_name} | è°ƒåº¦å™¨: {scheduler}")
                logger.info(f"ğŸ›ï¸ é‡ç»˜å¹…åº¦: {denoise} | æ­¥æ•°: {steps} | CFG: {cfg}")
                logger.info(f"ğŸ² ç§å­: {seed_info}")
                logger.info(f"ğŸ“Š èŠ‚ç‚¹æ€»æ•°: {total_nodes}")
                logger.info("ğŸ“¤ ***************************")

            except Exception as e:
                logger.warning(f"âš ï¸ å·¥ä½œæµæ¦‚è¦æ‰“å°å¤±è´¥: {e}")

        except Exception as e:
            logger.exception("âŒ å‚æ•°åˆå¹¶å¤±è´¥:")
            return jsonify({"code": 500, "msg": f"å‚æ•°åˆå¹¶å¤±è´¥: {str(e)}"}), 500

        try:
            result = proxy.send_to_comfyui(merged_workflow, client_id, comfyui_url)
            prompt_id = result["data"].get("prompt_id")

            if not prompt_id:
                logger.error("âŒ ComfyUI è¿”å›çš„ç»“æœä¸­æœªåŒ…å« prompt_id")
                return jsonify({"code": 500, "msg": "ComfyUIè¿”å›æ•°æ®å¼‚å¸¸"}), 500
            
            
            task_status[prompt_id] = {
                "type": "submitted",
                "data": {
                    "prompt_id": prompt_id,
                    "client_id": client_id,
                    "workflow_id": workflow_id,
                    "node_count": total_nodes  
                },
                "timestamp": time.time(),
                "enhanced": True
            }

           
            start_progress_tracker_by_mapping(prompt_id, workflow_id, client_id, comfyui_url)

            
            submit_message = {
                "type": "task_submitted",
                "data": {
                    "prompt_id": prompt_id,
                    "workflow_id": workflow_id,
                    "node_count": total_nodes,  
                    "client_id": client_id
                }
            }
            add_message_to_queue(client_id, submit_message)
            response_data = {
                "code": 0,
                "msg": "æäº¤æˆåŠŸ",
                "data": {
                    "prompt_id": prompt_id,
                    "taskId": prompt_id,
                    "number": result["data"].get("node_num", 0),
                    "client_id": client_id,
                    "node_num": total_nodes  # 
                }
            }
            
            #logger.info("âœ… ä»»åŠ¡æäº¤æˆåŠŸ")
            logger.info(f"  â†’ prompt_id: {prompt_id}")
            logger.info(f"  â†’ èŠ‚ç‚¹æ€»æ•°: {total_nodes}")
            
            return jsonify(response_data), 200

        except Exception as e:
            logger.exception("âŒ ComfyUIè¯·æ±‚å¤±è´¥:")
            return jsonify({"code": 500, "msg": f"ComfyUIè¯·æ±‚å¤±è´¥: {str(e)}"}), 500

    except Exception as e:
        logger.error(f"âŒ è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        return jsonify({"code": 500, "msg": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"}), 500


from flask import request
import json
import gevent
import time

@app.route("/ws")
def proxy_ws():
    """
    WebSocketä»£ç†æ¥å£
    å»ºç«‹å®¢æˆ·ç«¯ä¸æœåŠ¡å™¨é—´çš„WebSocketè¿æ¥ï¼Œç”¨äºå®æ—¶æ¶ˆæ¯æ¨é€
    æ›¿ä»£HTTPè½®è¯¢ï¼Œæä¾›æ›´é«˜æ•ˆçš„å®æ—¶é€šä¿¡
    
    è¯·æ±‚å‚æ•°:
        clientId: å®¢æˆ·ç«¯å”¯ä¸€æ ‡è¯†ç¬¦
    è¿”å›:
        WebSocketè¿æ¥: åŒå‘é€šä¿¡é€šé“
    """
    ws = request.environ.get("wsgi.websocket")
    if not ws:
        logger.warning("âŒ æ’ä»¶å‘æ¥çš„æ˜¯æ™®é€š HTTP è¯·æ±‚ï¼Œé WebSocket")
        return "Expected WebSocket", 400

    client_id = request.args.get("clientId") or request.headers.get("Clientid")
    if not client_id:
        logger.warning("âŒ æœªæä¾› clientIdï¼Œæ— æ³•å»ºç«‹ä¼ª WebSocket è½®è¯¢")
        ws.close()
        return


    try:
        while True:
            msgs = get_messages_for_client(client_id)

            if not msgs:
                logger.debug(f"ğŸ• [client {client_id}] å½“å‰æ— æ–°æ¶ˆæ¯")
            else:
                for msg in msgs:
                    ws.send(json.dumps(msg))
                    logger.info(f"ğŸ“¤ [client {client_id}] å·²è½¬å‘æ¶ˆæ¯: {msg}")

            gevent.sleep(1)  
    except Exception as e:
        logger.warning(f"âš ï¸ WebSocket å¼‚å¸¸: {e}")
    finally:
        ws.close()

@app.route('/api/config/comfyui_url', methods=['POST'])
def update_comfyui_url():
    data = request.get_json() or {}
    url = data.get('url')
    if not url:
        return jsonify({"code": 400, "msg": "missing url"}), 400
    url = sanitize_url(url)
    proxy.config['comfyui_url'] = url
    proxy.save_config()
    global COMFYUI_URL
    COMFYUI_URL = url
    return jsonify({"code": 200, "msg": "updated", "data": {"comfyuiUrl": url}})

CLOUD_CHECK_URL = "https://umanage.lightcc.cloud/prod-api/psPlus/workflow/checkOnline"

@app.route('/psPlus/workflow/checkOnline', methods=['GET'])
def check_online():
    """
    åœ¨çº¿çŠ¶æ€æ£€æŸ¥æ¥å£ï¼ˆæœ¬åœ°ä¼šè¯è¦†ç›–é€»è¾‘ï¼‰
    å¦‚æœåŒä¸€ç”¨æˆ·åç™»å½•è¦†ç›–æ—§ tokenï¼Œæ—§ token è¢«è¸¢å‡ºã€‚
    """
    logger.info("ğŸ” [CheckOnline] æ”¶åˆ°è¯·æ±‚")
    headers = dict(request.headers)
    logger.debug(f"[CheckOnline] Headers: {headers}")

    auth_header = headers.get("Authorization", "")
    token = auth_header.replace("Bearer ", "").strip()
    username = sessions.get(token)
    logger.debug(f"[CheckOnline] å½“å‰ token: {token}")
    logger.debug(f"[CheckOnline] å¯¹åº”ç”¨æˆ·å: {username}")
    logger.debug(f"[CheckOnline] å½“å‰ user_latest_token çŠ¶æ€: {user_latest_token}")

    if username:
        # æ£€æŸ¥æ˜¯å¦ä¸ºè¯¥ç”¨æˆ·çš„æœ€æ–° token
        latest_token = user_latest_token.get(username)
        if latest_token and latest_token != token:
            logger.warning(f"[CheckOnline] éæœ€æ–° token ç™»å½•å°è¯•ï¼Œæ‹’ç»è®¿é—®: {token[:8]}...")
            return jsonify({"code": 401, "msg": "è®¤è¯å¤±è´¥ï¼Œæ— æ³•è®¿é—®ç³»ç»Ÿèµ„æº", "data": None}), 401

        logger.info("âœ… åœ¨çº¿æ£€æŸ¥é€šè¿‡ - local")
        return jsonify({"code": 200, "msg": "åœ¨çº¿", "data": {"mode": "local", "user": username}})

    return jsonify({"code": 401, "msg": "è®¤è¯å¤±è´¥", "data": None}), 401


@app.route('/health', methods=['GET'])
def health_check():
    """
    æœåŠ¡å¥åº·æ£€æŸ¥æ¥å£
    æä¾›æœåŠ¡çŠ¶æ€ä¿¡æ¯ï¼Œç”¨äºç›‘æ§ç³»ç»Ÿæ£€æŸ¥æœåŠ¡å¯ç”¨æ€§
    è¿”å›æœåŠ¡ç‰ˆæœ¬ã€çŠ¶æ€å’Œæ”¯æŒçš„åŠŸèƒ½åˆ—è¡¨
    
    è¿”å›:
        json: åŒ…å«æœåŠ¡çŠ¶æ€å’Œå…ƒæ•°æ®çš„å“åº”
    """
    return jsonify({
        "status": "healthy",
        "service": "huiying-proxy-enhanced-fixed",
        "version": "2.5.0",
        "timestamp": datetime.now().isoformat(),
        "features": ["http_polling", "task_status", "message_queue", "enhanced_progress", "upload_progress", "mask_support"]
    })


CLOUD_AUTH_URL = "https://umanage.lightcc.cloud/prod-api/auth/login"

CLOUD_LOGOUT_URL = "https://umanage.lightcc.cloud/prod-api/auth/logout"
@app.route('/auth/login', methods=['POST'])
def login_compatible():
    data = request.get_json(silent=True)
    if data is None:
        data = request.form.to_dict()
    username = data.get("username")
    password = data.get("password")

    logger.info("ğŸ”‘ [Login] æ”¶åˆ°ç™»å½•è¯·æ±‚ï¼Œç”¨æˆ·å: %s", username)

    # æ¯æ¬¡ç™»å½•æ—¶é‡æ–°åŠ è½½æœ¬åœ°ç”¨æˆ·ï¼Œç¡®ä¿ç”¨æˆ·ä¿¡æ¯æœ€æ–°
    load_local_users()
    user = LOCAL_USERS.get(username)
    if user:
        logger.info("[Login] åœ¨æœ¬åœ°ç”¨æˆ·åˆ—è¡¨ä¸­æ‰¾åˆ°ç”¨æˆ· %s", username)
    else:
        logger.info("[Login] æœ¬åœ°ç”¨æˆ·åˆ—è¡¨ä¸­æœªæ‰¾åˆ°ç”¨æˆ· %s", username)

    if user:
        if user.get("password") == password:
            # æ¸…ç†æ—§ token å¹¶æ›´æ–°ä¸ºæœ€æ–°
            old_tokens = [t for t, u in sessions.items() if u == username]
            for t in old_tokens:
                del sessions[t]
                logger.info(f"ğŸ”„ æ—§ token æ¸…é™¤: {t[:8]}...")
            # ç”Ÿæˆæ–°token
            token = uuid.uuid4().hex
            sessions[token] = username
            user_latest_token[username] = token
            # ä¿å­˜ç”¨æˆ·ç™»å½•æ—¶çš„ headersï¼Œç”¨äº checkOnline æ ¡éªŒä½¿ç”¨
            user_latest_headers[username] = dict(request.headers)
            # æ›´æ–°æœ€åç™»å½•æ—¶é—´å¹¶ä¿å­˜
            user['last_login'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            LOCAL_USERS[username] = user
            save_local_users()
            logger.debug(f"[Login] ä¸ºç”¨æˆ· {username} å­˜å‚¨ token: {token}")
            logger.debug(f"[Login] ä¸ºç”¨æˆ· {username} å­˜å‚¨ headers: {user_latest_headers[username]}")
            logger.debug(f"[Login] å½“å‰ user_latest_token çŠ¶æ€: {user_latest_token}")
            logger.info("âœ… [Login] æœ¬åœ°è®¤è¯æˆåŠŸ (å·²æ¸…ç†æ—§ä¼šè¯)")
            return jsonify({
                "code": 200,
                "msg": "æ“ä½œæˆåŠŸ",
                "data": {
                    "scope": None,
                    "openid": None,
                    "access_token": token,
                    "refresh_token": None,
                    "expire_in": 604799,
                    "refresh_expire_in": None,
                    "client_id": data.get("clientId")
                }
            }), 200

        logger.warning("[Login] æœ¬åœ°å¯†ç ä¸åŒ¹é…")
        return jsonify({"code": 401, "msg": "å¯†ç é”™è¯¯"}), 401

    logger.info("[Login] æœ¬åœ°è®¤è¯å¤±è´¥ï¼Œå°è¯•äº‘ç«¯ç™»å½•")

    try:
        response = requests.post(
            CLOUD_AUTH_URL,
            json=data,
            timeout=proxy.config.get("timeout", 30)
        )
        logger.info("[Login] äº‘ç«¯è¿”å›çŠ¶æ€ç : %s", response.status_code)
        logger.debug("[Login] äº‘ç«¯è¿”å›å†…å®¹: %s", response.text)

        if response.status_code == 200:
            logger.info("âœ… [Login] äº‘ç«¯è¯·æ±‚æˆåŠŸ")
        else:
            logger.warning("âŒ [Login] äº‘ç«¯ç™»å½•å¤±è´¥")

        return Response(
            response.content,
            status=response.status_code,
            content_type=response.headers.get('Content-Type', 'application/json')
        )
    except requests.RequestException as e:
        logger.error("[Login] äº‘ç«¯è¯·æ±‚å¼‚å¸¸: %s", str(e))
        return jsonify({"code": 1, "msg": "request to cloud failed"}), 500

    
@app.route('/auth/logout', methods=['POST'])
def logout_proxy():
    """
    ç”¨æˆ·ç™»å‡ºæ¥å£
    æ”¯æŒæœ¬åœ°ä¼šè¯æ¸…é™¤å’Œäº‘ç«¯ç™»å‡ºï¼Œä¼˜å…ˆæ¸…é™¤æœ¬åœ°ä¼šè¯
    æœ¬åœ°ä¼šè¯ä¸å­˜åœ¨æ—¶è½¬å‘ç™»å‡ºè¯·æ±‚è‡³äº‘ç«¯æœåŠ¡
    
    è¯·æ±‚å¤´:
        Authorization: Bearer {token} - ç”¨æˆ·è®¤è¯ä»¤ç‰Œ
    è¿”å›:
        json: ç™»å‡ºç»“æœå“åº”
    """
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    logger.info(f"ğŸ”‘ [Logout] æ”¶åˆ°é€€å‡ºè¯·æ±‚ï¼Œtoken: {token}")
    if token in sessions:
        # æ¸…é™¤è¯¥ç”¨æˆ·çš„æœ€æ–° token è®°å½•
        username = sessions[token]
        del sessions[token]
        if username in user_latest_token:
            del user_latest_token[username]
        logger.info(f"âœ… [Logout] æœ¬åœ°é€€å‡ºæˆåŠŸ, ç”¨æˆ·: {username}")
        return jsonify({"code": 200, "msg": "æ“ä½œæˆåŠŸ", "data": None}), 200
    else:
        logger.info("[Logout] æœ¬åœ°ä¼šè¯ä¸å­˜åœ¨ï¼Œå°è¯•äº‘ç«¯ç™»å‡º")

    try:
        payload = request.get_data()
        headers = {
            key: value for key, value in request.headers.items()
            if key.lower() != 'host'
        }
        response = requests.post(
            CLOUD_LOGOUT_URL,
            headers=headers,
            data=payload,
            timeout=proxy.config.get("timeout", 30)
        )
        logger.info("[Logout] äº‘ç«¯è¿”å›çŠ¶æ€ç : %s", response.status_code)
        logger.debug("[Logout] äº‘ç«¯è¿”å›å†…å®¹: %s", response.text)
        try:
            result = response.json()
        except Exception:
            result = {"code": 500, "msg": "äº‘ç«¯å“åº”æ ¼å¼é”™è¯¯", "raw": response.text}
        if response.status_code == 200:
            logger.info("âœ… [Logout] é€€å‡ºæˆåŠŸ - by cloud")
        else:
            logger.warning("âŒ [Logout] é€€å‡ºå¤±è´¥ - by cloud")
        return jsonify(result), response.status_code
    except Exception as e:
        logger.error("[Logout] äº‘ç«¯è¯·æ±‚å¼‚å¸¸: %s", str(e))
        return jsonify({"code": 500, "msg": "ä»£ç†ç™»å‡ºå¤±è´¥", "error": str(e)}), 500

from gevent.pywsgi import WSGIServer
from geventwebsocket.handler import WebSocketHandler
if __name__ == '__main__':

    # monkey.patch_all() has already been called at the top of the file
    import logging
    

    logger.info("ğŸ”§ å¯åŠ¨ ComfyUI WebSocket ç›‘å¬çº¿ç¨‹...")
    logger.info("ğŸ”„ å·²ä½¿ç”¨å¢å¼ºHTTPè½®è¯¢æ¨¡å¼")
    comfy_ws_listener()

    logger.info("ğŸ”§ å¯åŠ¨æ¸…ç†ä»»åŠ¡çº¿ç¨‹æœåŠ¡")
    Thread(target=cleanup_task, daemon=True).start()
    

    port = proxy.config.get('proxy_port', 8080)
    server = WSGIServer(
        ("0.0.0.0", port),
        app,
        handler_class=WebSocketHandler,
        log=None
    )
    logger.info("âœ… HTTP & WebSocket æœåŠ¡å¯åŠ¨æˆåŠŸ")
    logger.info(f"ğŸŸ¢ ä»£ç†æœåŠ¡å¯åŠ¨ï¼Œç›‘å¬ç«¯å£: {port}")
    logger.info(
        f"âœ… å·¥ä½œæµæ˜ å°„é…ç½®åŠ è½½å®Œæˆï¼Œå·¥ä½œæµæ•°é‡: {len(proxy.mappings.get('workflow_mappings', {}))}"
    )
    print("============== æ¬¢è¿ä½¿ç”¨ç»˜å½± AICG ä»£ç†ç»ˆç«¯æœåŠ¡ v2.5  ==============")

    server.serve_forever()

