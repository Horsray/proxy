#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
V2.5 
"""
from gevent import monkey
monkey.patch_all()
import os
import json
import requests
import time
import uuid
import copy
import logging
logging.getLogger("urllib3.connectionpool").setLevel(logging.WARNING)
# Path to local users file relative to this script
USERS_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "users.json")
sessions = {}

def load_local_users():
    """Load local user credentials from users.json."""
    if os.path.exists(USERS_FILE):
        try:
            with open(USERS_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
            logging.info(f"ğŸ” å·²åŠ è½½æœ¬åœ°ç”¨æˆ·æ–‡ä»¶: {USERS_FILE}")
            logging.debug(f"æœ¬åœ°ç”¨æˆ·åˆ—è¡¨: {list(data.get('users', {}).keys())}")
            return data.get("users", {})
        except Exception as e:
            logging.error(f"âŒ æœ¬åœ°ç”¨æˆ·æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
    else:
        logging.warning(f"âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°ç”¨æˆ·æ–‡ä»¶: {USERS_FILE}")
    return {}

from datetime import datetime, timedelta
from threading import Thread, Lock
from flask import Flask, request, jsonify, Response
from flask_cors import CORS
import websocket as ws_client
from collections import defaultdict, deque
# Default ComfyUI URL, will be overwritten by config on start
COMFYUI_URL = ""

logging.basicConfig(
    level=logging.DEBUG, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('huiying_proxy.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

LOCAL_USERS = load_local_users()


app = Flask(__name__)
CORS(app, origins="*")


def sanitize_url(url: str) -> str:
    """Normalize user-provided URLs for requests."""
    if not url:
        return url
    return url.replace("\\", "/").rstrip('/')


def is_remote_url(url: str) -> bool:
    """Return True if the given URL points to a public (non-local) address."""
    try:
        from urllib.parse import urlparse
        import ipaddress

        host = urlparse(url).hostname
        if not host:
            return False
        try:
            ip = ipaddress.ip_address(host)
            return not (ip.is_private or ip.is_loopback)
        except ValueError:
            # Host is not an IP address; assume remote
            return True
    except Exception:
        return False


def adapt_workflow_paths(workflow_data, comfyui_url: str):
    """Convert path separators for remote ComfyUI servers."""
    if not is_remote_url(comfyui_url):
        return workflow_data

    def _convert(obj):
        if isinstance(obj, dict):
            return {k: _convert(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [_convert(v) for v in obj]
        elif isinstance(obj, str):
            return obj.replace("\\", "/")
        else:
            return obj

    return _convert(workflow_data)


comfyui_ws = None  
message_queue = defaultdict(deque) 
task_status = {}  
client_last_seen = {}  
upload_progress = {}  
queue_lock = Lock()  
task_result_cache = {}

def start_progress_tracker_by_mapping(prompt_id, workflow_id, client_id, comfyui_url):
    comfyui_url = sanitize_url(comfyui_url)
    try:
        with open("workflow_mappings.json", "r", encoding="utf-8") as f:
            mappings = json.load(f).get("workflow_mappings", {})
            total_nodes = mappings.get(workflow_id, {}).get("node_count", 20)  # é»˜è®¤ 20
    except Exception as e:
        logger.warning(f"âš ï¸ è·å–å·¥ä½œæµèŠ‚ç‚¹æ€»æ•°å¤±è´¥: {e}")
        total_nodes = 20

    def track():
        max_poll = 120
        for i in range(1, max_poll + 1):
            try:
                url = f"{comfyui_url}/history/{prompt_id}"
                resp = requests.get(url, timeout=3)
                if resp.status_code != 200:
                    logger.debug(f"è½®è¯¢ {i} æ¬¡ - ComfyUI è¿”å›çŠ¶æ€ç : {resp.status_code}")
                    print(f"\rDEBUG  -  ğŸ¯ æ­£åœ¨ç­‰å¾…ä»»åŠ¡å®Œæˆ... å·²è½®è¯¢ {i} æ¬¡ï¼Œå°šæœªè·å–åˆ°å†å²è®°å½•", end="", flush=True)
                    time.sleep(1)
                    continue

                data = resp.json()
                if prompt_id not in data:
                    print(f"\rDEBUG  -  ğŸ¯ æ­£åœ¨ç­‰å¾…ä»»åŠ¡å®Œæˆ... å·²è½®è¯¢ {i} æ¬¡ï¼Œå°šæ— è¯¥ä»»åŠ¡è®°å½•", end="", flush=True)
                    time.sleep(1)
                    continue

                history_data = data[prompt_id]
                status = history_data.get("status", {})

                outputs = history_data.get("outputs", {})
                current_node = len(outputs)

                if status.get("status_str") == "success":
                    current_node = total_nodes
                elif status.get("status_str") == "error":
                    current_node = 0

                percent = int((current_node / max(1, total_nodes)) * 100)

                print(f"\rDEBUG  -  ğŸ¯ æ­£åœ¨ç­‰å¾…ä»»åŠ¡å®Œæˆ... å·²è½®è¯¢ {i} æ¬¡ï¼Œè¿›åº¦ {percent}% [èŠ‚ç‚¹ {current_node}/{total_nodes}]", end="", flush=True)

                
                add_message_to_queue(client_id, {
                    "type": "executing",        
                    "level": "info",           
                    "data": {
                        "prompt_id":     prompt_id,
                        "workflow_id":   workflow_id,
                        "node":          current_node    
                    }
                })

                add_message_to_queue(client_id, {
                    "type": "progress",          
                    "level": "info",             
                    "data": {
                        "prompt_id":      prompt_id,
                        "workflow_id":    workflow_id,
                        "value":          current_node,    
                        "max":            total_nodes,     
                        "percentage":     percent,         
                        "sampler_step":   sampler_step,    
                        "sampler_steps":  sampler_steps
                    }
                })
                if current_node >= total_nodes or status.get("status_str") in ["success", "error"]:
                    print()  # å®Œæˆåæ¢è¡Œ
                    logger.info(f"ğŸ“ˆ è¿›åº¦æ›´æ–°: {percent}% [èŠ‚ç‚¹ {current_node}/{total_nodes}]")
                    break

            except Exception as e:
                print(f"\rDEBUG  -  ğŸ¯ ç¬¬ {i} æ¬¡è½®è¯¢å¼‚å¸¸: {e}", end="", flush=True)
            time.sleep(1)


def add_message_to_queue(client_id, message):
   
    with queue_lock:
    
        if len(message_queue[client_id]) > 100:
            message_queue[client_id].popleft()
     
        enhanced_message = {
            "id": str(uuid.uuid4())[:8],
            "timestamp": time.time(),
            "data": message
        }
        
        message_queue[client_id].append(enhanced_message)
        logger.info(f"ğŸ“¨ æ¶ˆæ¯å·²æ·»åŠ åˆ°å®¢æˆ·ç«¯é˜Ÿåˆ—: {client_id} (ç±»å‹: {message.get('type', 'unknown')})")

def get_messages_for_client(client_id, since_timestamp=None):
    
    with queue_lock:
        client_last_seen[client_id] = time.time()
        
        if client_id not in message_queue:
            return []
        
        messages = []
        for msg in message_queue[client_id]:
            if since_timestamp is None or msg["timestamp"] > since_timestamp:
                messages.append(msg)
        
        # æ¸…ç†å·²è¯»æ¶ˆæ¯ï¼ˆä¿ç•™æœ€è¿‘20æ¡ï¼‰
        if len(messages) > 0:
            message_queue[client_id] = deque(list(message_queue[client_id])[-20:])
        
        return messages

def broadcast_message(message):
   
    current_time = time.time()
    active_clients = []
    
    with queue_lock:
        # æ‰¾å‡ºæ´»è·ƒå®¢æˆ·ç«¯ï¼ˆæœ€è¿‘5åˆ†é’Ÿå†…æœ‰æ´»åŠ¨ï¼‰
        for client_id, last_seen in client_last_seen.items():
            if current_time - last_seen < 300:  # 5åˆ†é’Ÿ
                active_clients.append(client_id)
    

    for client_id in active_clients:
        add_message_to_queue(client_id, message)

def cleanup_inactive_clients():

    current_time = time.time()
    inactive_clients = []
    
    with queue_lock:
        for client_id, last_seen in client_last_seen.items():
            if current_time - last_seen > 600:  # 10åˆ†é’Ÿæ— æ´»åŠ¨
                inactive_clients.append(client_id)
        
        for client_id in inactive_clients:
            del client_last_seen[client_id]
            if client_id in message_queue:
                del message_queue[client_id]
            if client_id in upload_progress:
                del upload_progress[client_id]
    
    if inactive_clients:
        logger.info(f"ğŸ§¹ æ¸…ç†äº† {len(inactive_clients)} ä¸ªéæ´»è·ƒå®¢æˆ·ç«¯")


def comfy_ws_listener():
    import websocket
    import json
    import copy

    def enhance_message(original_message):
        enhanced = copy.deepcopy(original_message)
        msg_type = enhanced.get("type")
        data = enhanced.get("data", {})

        if msg_type == "status":
            status = data.get("status", {})
            exec_info = status.get("exec_info", {})
            enhanced["data"]["enhanced_info"] = {
                "queue_remaining": exec_info.get("queue_remaining", 0),
                "queue_running": len(exec_info.get("queue_running", [])),
                "timestamp": time.time()
            }

        elif msg_type == "executing":
            node_id = data.get("node")
            prompt_id = data.get("prompt_id")
            enhanced["data"]["enhanced_info"] = {
                "node_id": node_id,
                "prompt_id": prompt_id,
                "status": "executing",
                "timestamp": time.time()
            }

        elif msg_type == "progress":
            value = data.get("value", 0)
            max_value = data.get("max", 0)
            node_id = data.get("node")
            prompt_id = data.get("prompt_id")
            try:
                percentage = round((value / max_value) * 100, 1) if max_value else 0
            except:
                percentage = 0
            enhanced["data"]["enhanced_info"] = {
                "percentage": percentage,
                "current_step": value,
                "total_steps": max_value,
                "node_id": node_id or "unknown",
                "prompt_id": prompt_id or "unknown",
                "is_sampling": str(data.get("name", "")).lower().startswith("ksampler"),
                "timestamp": time.time()
            }

        elif msg_type == "executed":
            prompt_id = data.get("prompt_id")
            node_id = data.get("node")
            enhanced["data"]["enhanced_info"] = {
                "prompt_id": prompt_id,
                "node_id": node_id,
                "status": "completed",
                "timestamp": time.time()
            }

        return enhanced

    def on_message(ws, message):
        try:
            msg_json = json.loads(message)
            enhanced = enhance_message(msg_json)

            msg_type = msg_json.get("type")
            data = msg_json.get("data", {})

            if msg_type == "progress":
                prompt_id = data.get("prompt_id")
                value = data.get("value", 0)
                max_value = data.get("max", 1)
                percent = int((value / max_value) * 100) if max_value else 0
                node_id = data.get("node")

                if prompt_id in task_status:
                    task_status[prompt_id]["type"] = "progress"
                    task_status[prompt_id]["data"].update({
                        "percentage": percent,
                        "current_step": value,
                        "total_steps": max_value,
                        "node_id": node_id,
                        "is_sampling": str(data.get("name", "")).lower().startswith("ksampler"),
                    })
                    task_status[prompt_id]["timestamp"] = time.time()
                    logger.info(f"ğŸ“ˆ è¿›åº¦æ›´æ–°: {percent}% [{value}/{max_value}] @èŠ‚ç‚¹ {node_id}")

            elif msg_type == "executing":
                prompt_id = data.get("prompt_id")
                node_id = data.get("node")

                if prompt_id in task_status:
                    task_status[prompt_id]["type"] = "executing"
                    task_status[prompt_id]["data"].update({
                        "node_id": node_id,
                        "status": "executing"
                    })
                    task_status[prompt_id]["timestamp"] = time.time()
                    logger.info(f"âš™ï¸ [æ‰§è¡Œä¸­] {prompt_id} èŠ‚ç‚¹: {node_id}")

            elif msg_type == "executed":
                prompt_id = data.get("prompt_id")

                if prompt_id in task_status:
                    task_status[prompt_id]["type"] = "done"
                    task_status[prompt_id]["data"].update({
                        "status": "done"
                    })
                    task_status[prompt_id]["timestamp"] = time.time()
                    logger.info(f"âœ… [å®Œæˆ] {prompt_id}")

        
            client_id = task_status.get(data.get("prompt_id"), {}).get("data", {}).get("client_id")
            if client_id:
                add_message_to_queue(client_id, enhanced)

        except Exception as e:
            logger.warning(f"âš ï¸ WebSocketæ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
    logging.getLogger("websocket").setLevel(logging.CRITICAL)
    def on_error(ws, error):
        logger.error(f"è¿œç¨‹ç›‘å¬æœåŠ¡å°šæœªå¼€å¯ï¼Œè¯·ç­‰å¾…...")
    def on_close(ws, close_status_code, close_msg):
        logger.warning(f"ç­‰å¾…æ ¸å¿ƒæœåŠ¡ç¨‹åºå¯åŠ¨ï¼Œå¼€å§‹å°è¯•å»ºç«‹è¿æ¥")
        
    def on_open(ws):
        logger.info("ğŸ”— [ComfyUI WS] è¿æ¥å·²å»ºç«‹")
def start_ws_listener(base_url):
    if not base_url or not base_url.startswith("http"):
        print("âš ï¸ è·³è¿‡ WebSocket åˆå§‹åŒ–ï¼šæ— æœ‰æ•ˆ URL")
        return

    import websocket

    ws_url = base_url.replace("http://", "ws://").replace("https://", "wss://") + "/ws"
    ws = websocket.WebSocketApp(
        ws_url,
        on_message=on_message,
        on_error=on_error,
        on_close=on_close,
        on_open=on_open
    )

    thread = Thread(target=ws.run_forever, daemon=True)
    thread.start()


def cleanup_task():
   
    while True:
        try:
            cleanup_inactive_clients()
            
           
            current_time = time.time()
            expired_tasks = []
            for prompt_id, status_info in task_status.items():
                if current_time - status_info["timestamp"] > 7200:  # 2å°æ—¶
                    expired_tasks.append(prompt_id)
            
            for prompt_id in expired_tasks:
                del task_status[prompt_id]
            
            if expired_tasks:
                logger.info(f"ğŸ§¹ æ¸…ç†äº† {len(expired_tasks)} ä¸ªè¿‡æœŸä»»åŠ¡çŠ¶æ€")
                
        except Exception as e:
            logger.error(f"æ¸…ç†ä»»åŠ¡å¼‚å¸¸: {e}")
        
        time.sleep(60)  

 
class HuiYingProxy:
    def __init__(self, config_file='config.json', mappings_file='workflow_mappings.json'):
        self.config_file = config_file
        self.mappings_file = mappings_file
        self.config = self.load_config(config_file)
        self.mappings = self.load_mappings(mappings_file)
        self.workflow_cache = {}
        self.workflow_node_count = {}
        if self.config.get('load_from_cloud'):
            self.preload_workflows()

    def save_config(self):
        """Persist current configuration to disk."""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=2, ensure_ascii=False)
            logger.info("ğŸ’¾ é…ç½®å·²ä¿å­˜")
        except Exception as e:
            logger.warning(f"âš ï¸ é…ç½®ä¿å­˜å¤±è´¥: {e}")
        

    def load_config(self, config_file):
        
        
        default_config = {
            "workflow_dir": "workflows",
            "comfyui_url": "",
            "cloud_service_url": "proxy.hueying.cn",
            "proxy_port": 8080,
            "load_from_cloud": False,
            "timeout": 30,
            "enable_parameter_validation": True,
            "enable_workflow_cache": True,
            "log_level": "INFO"
        }
        logger.info(f"ğŸ“ å¼€å§‹æ‰«ææ‰€éœ€çš„å¿…è¦æ–‡ä»¶")

        if os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                default_config.update(user_config)
                logger.info(f"âœ… åŸºç¡€é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_file}")
    
            except Exception as e:
                logger.error(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        else:
        
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ†• åˆå§‹åŒ–é»˜è®¤é…ç½®æ–‡ä»¶æˆåŠŸ: {config_file}")
            logger.info(f"ğŸ’¾ å·²ä¿å­˜ ComfyUI åœ°å€é…ç½®: {default_config['comfyui_url']}")
  
        default_config["workflow_dir"] = os.path.abspath(default_config["workflow_dir"])
        default_config["comfyui_url"] = sanitize_url(default_config["comfyui_url"])
        if default_config.get('load_from_cloud'):
            logger.info("ğŸ“ å°†ä»äº‘ç«¯åŠ è½½å·¥ä½œæµå’Œæ˜ å°„é…ç½®")
        else:
            logger.info(f"ğŸ“ å½“å‰å·¥ä½œæµè·¯å¾„ä¸º: {default_config['workflow_dir']}")
        logger.info(f"ğŸ”— å½“å‰ ComfyUI åœ°å€: {default_config['comfyui_url']}")


        return default_config

    def load_mappings(self, mappings_file):
        if self.config.get('load_from_cloud'):
            return self.fetch_remote_mappings()

        if not os.path.exists(mappings_file):
            logger.warning(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {mappings_file}")
            return {}
        
        try:
            with open(mappings_file, 'r', encoding='utf-8') as f:
                try:
                    mappings = json.load(f)
                    logger.info(f"âœ… åˆå§‹åŒ–å‚æ•°æ˜ å°„é…ç½®æˆåŠŸ: {mappings_file}")
                    return mappings
                except json.JSONDecodeError as e:
                    logger.error(f"ğŸ”¥ JSONè¯­æ³•é”™è¯¯: {e.msg}ï¼Œä½ç½®ï¼šè¡Œ{e.lineno}åˆ—{e.colno}")
                    with open(mappings_file, 'r', encoding='utf-8') as f_context:
                        lines = f_context.readlines()
                        context = lines[max(0, e.lineno-2):e.lineno+1]
                        logger.error(f"ğŸ”¥ é”™è¯¯è¡Œä¸Šä¸‹æ–‡:\n{''.join(context)}")
                    return {}
        except Exception as e:
            logger.error(f"ğŸ”¥ é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
            return {}

    def load_workflow(self, workflow_id):


        if self.config.get('load_from_cloud'):
            if workflow_id in self.workflow_cache:
                logger.info(f"âš¡ ä»ç¼“å­˜ä¸­åŠ è½½äº‘ç«¯å·¥ä½œæµ: {workflow_id}")
                return self.workflow_cache[workflow_id]
            return self.fetch_remote_workflow(workflow_id)

        if self.config.get('enable_workflow_cache', True) and workflow_id in self.workflow_cache:
            logger.info(f"âš¡ ä»ç¼“å­˜ä¸­åŠ è½½å½“å‰å·¥ä½œæµ: {workflow_id}")
            return self.workflow_cache[workflow_id]
            
        workflow_file = os.path.join(self.config['workflow_dir'], f"{workflow_id}.json")
        
        if not os.path.exists(workflow_file):
            logger.error(f"âŒ å·¥ä½œæµæ–‡ä»¶ä¸å­˜åœ¨: {workflow_file}")
            raise FileNotFoundError(f"å·¥ä½œæµæ–‡ä»¶ä¸å­˜åœ¨: {workflow_file}")
            
        try:
            with open(workflow_file, 'r', encoding='utf-8') as f:
                workflow = json.load(f)
     
            if self.config.get('enable_workflow_cache', True):
                self.workflow_cache[workflow_id] = workflow
            
            logger.info(f"ğŸ“„ ä»ç¼“å­˜ä¸­è¯»å–åˆ°å½“å‰å·¥ä½œæµ: {workflow_id}")
            return workflow
            
        except Exception as e:
            logger.error(f"å·¥ä½œæµåŠ è½½å¤±è´¥ {workflow_id}: {e}")
            raise
      
    def _set_nested_value(self, data, path, value):

        try:
            for key in path[:-1]:
                if isinstance(key, int):
                    data = data[key]
                else:
                    data = data.setdefault(key, {})
            last_key = path[-1]
            if isinstance(last_key, int):
                data[last_key] = value
            else:
                data[last_key] = value
            logger.debug(f"âœ… è®¾ç½®è·¯å¾„ {path} = {value}")
        except Exception as e:
            logger.error(f"âŒ è®¾ç½®å‚æ•°å¤±è´¥: path={path}, value={value}, é”™è¯¯: {e}")

    def fetch_remote_mappings(self):
        cloud_url = sanitize_url(self.config.get('cloud_service_url', ''))
        url = f"{cloud_url}/workflow_mappings.json"
        try:
            resp = requests.get(url, timeout=self.config.get('timeout', 30))
            resp.raise_for_status()
            logger.info(f"âœ… ä»äº‘ç«¯åŠ è½½æ˜ å°„é…ç½®æˆåŠŸ: {url}")
            return resp.json()
        except Exception as e:
            logger.error(f"âŒ åŠ è½½äº‘ç«¯æ˜ å°„é…ç½®å¤±è´¥: {e}")
            return {}

    def fetch_remote_workflow(self, workflow_id):
        cloud_url = sanitize_url(self.config.get('cloud_service_url', ''))
        url = f"{cloud_url}/workflows/{workflow_id}.json"
        try:
            resp = requests.get(url, timeout=self.config.get('timeout', 30))
            resp.raise_for_status()
            workflow = resp.json()
            self.workflow_cache[workflow_id] = workflow
            logger.info(f"âœ… ä»äº‘ç«¯åŠ è½½å·¥ä½œæµ: {workflow_id}")
            return workflow
        except Exception as e:
            logger.error(f"âŒ è·å–äº‘ç«¯å·¥ä½œæµ {workflow_id} å¤±è´¥: {e}")
            return {}

    def preload_workflows(self):
        workflow_ids = self.mappings.get('workflow_mappings', {}).keys()
        for wid in workflow_ids:
            self.fetch_remote_workflow(wid)

    def merge_workflow_params(self, workflow, param_dict, workflow_id):
      
        try:
            merged_workflow = copy.deepcopy(workflow)

            workflow_mappings = self.mappings.get('workflow_mappings', {})
            param_mappings = workflow_mappings.get(workflow_id, {}).get('param_mappings', {})

            logger.info(f"ğŸ”§ åŒ¹é…åˆ°ç»˜å½± AIGC å‘é€çš„ {len(param_dict)} ä¸ªå‚æ•°")

            for param_key, param_value in param_dict.items():
                if isinstance(param_value, str) and param_value.startswith("é»˜è®¤"):
                    #logger.info(f"ğŸ†— é»˜è®¤å‚æ•°: {param_key} = {param_value}")
                    continue
                if param_key not in param_mappings:
                    logger.debug(f"â­ï¸ æœªæ˜ å°„å‚æ•°: {param_key}")
                    continue

                path = param_mappings[param_key]
                self._set_nested_value(merged_workflow, path, param_value)

            return merged_workflow

        except Exception as e:
            logger.error(f"âŒ å‚æ•°åˆå¹¶å¤±è´¥: {e}")
            raise
       
    def send_to_comfyui(self, workflow_data, client_id, comfyui_url=None):

        import requests

        if not comfyui_url:
            comfyui_url = self.config.get("comfyui_url")
        if not comfyui_url:
            raise ValueError("comfyui_url is required")
        comfyui_url = sanitize_url(comfyui_url)
        workflow_data = adapt_workflow_paths(workflow_data, comfyui_url)
        url = f"{comfyui_url}/prompt"

        try:
            headers = {
                "Content-Type": "application/json"
            }
            payload = {
                "client_id": client_id,
                "prompt": workflow_data
            }
            logger.info(f"ğŸš€ æ­£åœ¨æäº¤ä»»åŠ¡åˆ° ç”ŸæˆæœåŠ¡å™¨: {url}")
            response = requests.post(url, json=payload, headers=headers, timeout=self.config.get("timeout", 30))

            if response.status_code == 200:
                logger.info("âœ… ä»»åŠ¡æäº¤æˆåŠŸ")
                return {"data": response.json()}
            else:
                logger.error(f"âŒ ä»»åŠ¡è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, å†…å®¹: {response.text}")
                return {"error": f"ä»»åŠ¡ è¯·æ±‚å¤±è´¥: {response.status_code}", "detail": response.text}

        except Exception as e:
            logger.error(f"âŒ ComfyUIè¯·æ±‚å¤±è´¥: {str(e)}")
            return {"error": str(e)}
proxy = HuiYingProxy()
COMFYUI_URL = sanitize_url(proxy.config.get("comfyui_url", ""))


@app.before_request
def log_all_requests():
    logger.info(f"ğŸ“¡ æ”¶åˆ°æ’ä»¶æ¥å£è¯·æ±‚: {request.method} {request.path}")

# å¤„ç†è·¨åŸŸè¯·æ±‚
@app.route('/api/poll', methods=['GET'])
def poll_messages():

    client_id = request.args.get('clientId')
    since_timestamp = request.args.get('since', type=float)
    
    if not client_id:
        return jsonify({"error": "ç¼ºå°‘clientIdå‚æ•°"}), 400
    
    try:
        messages = get_messages_for_client(client_id, since_timestamp)
        

        extra_info = {
            "active_tasks": len(task_status),
            "queue_size": len(message_queue.get(client_id, [])),
            "server_time": time.time()
        }
        
        return jsonify({
            "code": 0,
            "msg": "success",
            "data": {
                "messages": messages,
                "timestamp": time.time(),
                "clientId": client_id,
                "extra_info": extra_info
            }
        })
        
    except Exception as e:
        logger.error(f"è½®è¯¢æ¶ˆæ¯å¤±è´¥: {e}")
        return jsonify({"error": f"è½®è¯¢å¤±è´¥: {str(e)}"}), 500

@app.route('/api/task_status/<prompt_id>', methods=['GET'])
def get_task_status(prompt_id):

    try:
        if prompt_id in task_status:
            status_info = task_status[prompt_id]
         
            enhanced_status = {
                **status_info,
                "age_seconds": time.time() - status_info["timestamp"],
                "is_recent": (time.time() - status_info["timestamp"]) < 300  # 5åˆ†é’Ÿå†…
            }
            
            return jsonify({
                "code": 0,
                "msg": "success",
                "data": enhanced_status
            })
        else:
            return jsonify({
                "code": 404,
                "msg": "ä»»åŠ¡çŠ¶æ€ä¸å­˜åœ¨",
                "data": None
            }), 404
            
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({"error": f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}"}), 500
# #comfyuiå¯¹è±¡ä¿¡æ¯æ¥å£
# @app.route('/api/object_info', methods=['GET'])
# def proxy_object_info():
  
#     try:
#         comfyui_url = request.args.get('comfyuiUrl') or proxy.config.get('local_comfyui_url', COMFYUI_URL)
#         comfyui_url = sanitize_url(comfyui_url)
#         res = requests.get(f"{comfyui_url}/object_info", timeout=10)
#         logger.info("ğŸ” æˆåŠŸè½¬å‘ object_infoï¼ŒçŠ¶æ€ç : %s", res.status_code)
#         return Response(
#             res.content,
#             status=res.status_code,
#             content_type=res.headers.get('Content-Type', 'application/json')
#         )
#     except Exception as e:
#         logger.error(f"âŒ object_info è½¬å‘å¤±è´¥: {e}")
#         return jsonify({"error": f"è¿æ¥å¤±è´¥: {str(e)}"}), 500
# #å›¾åƒä¸Šä¼ æ¥å£
# @app.route('/upload/image', methods=['POST'])
# def proxy_upload_image():
#     logger.info("ğŸ–¼ï¸ æ”¶åˆ°æ’ä»¶ä¸Šä¼ å›¾åƒè¯·æ±‚ï¼Œå¼€å§‹è½¬å‘ç»™çœŸå® ComfyUI")
#     try:
#         data = request.form.to_dict()
#         files = {
#             key: (f.filename, f.stream, f.mimetype)
#             for key, f in request.files.items()
#         }
#         comfyui_url = data.get('comfyuiUrl') or proxy.config.get('local_comfyui_url', COMFYUI_URL)
#         comfyui_url = sanitize_url(comfyui_url)
#         resp = requests.post(f"{comfyui_url}/upload/image", data=data, files=files)
#         logger.info(f"ğŸ“¤ æˆåŠŸè½¬å‘å›¾åƒä¸Šä¼ è¯·æ±‚ï¼ŒçŠ¶æ€ç : {resp.status_code}")
#         return (resp.content, resp.status_code, resp.headers.items())
#     except Exception as e:
#         logger.exception("âŒ è½¬å‘å›¾åƒä¸Šä¼ å¤±è´¥:")
#         return jsonify({"code": 500, "msg": "å›¾åƒä¸Šä¼ è½¬å‘å¤±è´¥", "error": str(e)}), 500

# #è’™ç‰ˆæ¥å£ï¼ˆé¢„ç•™ï¼‰
# @app.route('/api/upload/mask', methods=['POST'])
# def proxy_upload_mask():
#     logger.info("ğŸ–¤ æ”¶åˆ°æ’ä»¶ä¸Šä¼  mask è¯·æ±‚ï¼Œå¼€å§‹è½¬å‘ç»™çœŸå® ComfyUI")
#     try:
#         data = request.form.to_dict()
#         files = {
#             key: (f.filename, f.stream, f.mimetype)
#             for key, f in request.files.items()
#         }
#         comfyui_url = data.get('comfyuiUrl') or proxy.config.get('local_comfyui_url', COMFYUI_URL)
#         comfyui_url = sanitize_url(comfyui_url)
#         resp = requests.post(f"{comfyui_url}/upload/mask", data=data, files=files)
#         logger.info(f"ğŸ“¤ æˆåŠŸè½¬å‘ mask ä¸Šä¼ è¯·æ±‚ï¼ŒçŠ¶æ€ç : {resp.status_code}")
#         return (resp.content, resp.status_code, resp.headers.items())
#     except Exception as e:
#         logger.exception("âŒ mask è½¬å‘å¤±è´¥:")
#         return jsonify({"code": 500, "msg": "ä¸Šä¼  mask è½¬å‘å¤±è´¥", "error": str(e)}), 500
#æ•°æ®æäº¤æ¥å£
@app.route('/psPlus/workflow/huiYingCommit', methods=['POST'])
def huiying_commit():

    data = request.get_json() or {}

    # logger.debug(f"ğŸŒ æ”¶åˆ°å®Œæ•´æ•°æ®: {json.dumps(data, indent=2, ensure_ascii=False)}")
    # logger.info("ğŸ› ï¸ æ­£åœ¨å¤„ç†ç»˜å½±å·¥ä½œæµæäº¤è¯·æ±‚...")

    # try:
    #     #logger.debug(f"ğŸ” Headers: {dict(request.headers)}")

    #     if request.is_json:
    #         #logger.debug(f"ğŸ§¾ JSON Body: {request.get_json()}")
    #     else:
    #         #logger.debug(f"ğŸ§¾ é JSON è¯·æ±‚ä½“")
    # except Exception as e:
    #     #logger.error(f"âŒ æ‰“å°è¯·æ±‚å¤±è´¥: {e}")

    try:
      
        logger.info("ğŸ“ æ¥æ”¶æ¥è‡ª ç»˜å½± AI çš„æ•°æ®å­—æ®µ")
        if not data:
            return jsonify({"code": 400, "msg": "è¯·æ±‚æ•°æ®ä¸ºç©º"}), 400
        
        workflow_id = data.get('workflowId')
        param_dict = data.get('paramDict', {})
        client_id = data.get('clientId', str(uuid.uuid4()))
        comfyui_url = data.get('comfyuiUrl') or proxy.config.get('comfyui_url')
        if not comfyui_url:
            return jsonify({"code": 400, "msg": "missing comfyuiUrl"}), 400
        comfyui_url = sanitize_url(comfyui_url)
        
       
        if not workflow_id:
            return jsonify({"code": 400, "msg": "workflowIdä¸èƒ½ä¸ºç©º"}), 400
        
        
        try:
            workflow = proxy.load_workflow(workflow_id)
            logger.info(f"ğŸ“¦ å·¥ä½œæµåŠ è½½æˆåŠŸ: {workflow_id}")
            logger.info(f"ğŸ“Š å­˜åœ¨æ€»èŠ‚ç‚¹æ•°: {len(workflow)}")
            logger.info(f"ğŸ“¥ æ¥æ”¶å‚æ•°æ•°é‡: {len(param_dict)}")
            
           
            for k, v in param_dict.items():
                logger.debug(f"  â”œâ”€ å‚æ•°: {k} = {v}")
                
        except FileNotFoundError:
            return jsonify({"code": 404, "msg": f"å·¥ä½œæµä¸å­˜åœ¨: {workflow_id}"}), 404
        except Exception as e:
            return jsonify({"code": 500, "msg": f"å·¥ä½œæµåŠ è½½å¤±è´¥: {str(e)}"}), 500
        
      
        try:
            merged_workflow = proxy.merge_workflow_params(workflow, param_dict, workflow_id)

            
            total_nodes = len([
                k for k, v in merged_workflow.items()
                if isinstance(v, dict) and "class_type" in v
            ])

            logger.info(f"ğŸ“Š å‚æ•°åˆå¹¶æ ¡éªŒå®Œæ¯• ")

          
            valid_workflow = {}
            for key, value in merged_workflow.items():
                if isinstance(value, dict) and "class_type" in value:
                    valid_workflow[key] = value
                else:
                    logger.warning(f"âš ï¸ ç§»é™¤éæ³•èŠ‚ç‚¹: {key}")
            merged_workflow = valid_workflow

            try:
                BASE_DIR = os.path.dirname(os.path.abspath(__file__))
                model_name = None
                sampler_name = "N/A"
                scheduler = "N/A"
                steps = "N/A"
                cfg = "N/A"
                denoise = "N/A"
                seed = "N/A"

                has_checkpoint = False

                for node_id, node in merged_workflow.items():
                    if not isinstance(node, dict):
                        continue
                    inputs = node.get("inputs", {})
                    class_type = node.get("class_type", "")

                    if class_type == "CheckpointLoaderSimple":
                        model_name = inputs.get("ckpt_name")
                        has_checkpoint = True
                    elif class_type == "UNetLoader" and not has_checkpoint:
                        model_name = "UNET åŠå…¶ä»–"

                    if class_type in ["KSampler", "KSamplerAdvanced"]:
                        sampler_name = inputs.get("sampler_name", sampler_name)
                        scheduler = inputs.get("scheduler", scheduler)
                        steps = inputs.get("steps", steps)
                        cfg = inputs.get("cfg", cfg)
                        denoise = inputs.get("denoise", denoise)
                        seed = inputs.get("seed", seed)

                    if "image" in inputs:
                        image_path = inputs["image"]
                        width = inputs.get("width", "æœªçŸ¥")
                        height = inputs.get("height", "æœªçŸ¥")
                        image_size = f"{width}x{height}"


                seed_info = f"{seed}ï¼ˆéšæœºï¼‰" if str(seed) in ["-1", "None", "-1.0"] else str(seed)
                model_display = model_name if model_name else "UNET åŠå…¶ä»–"

                logger.info("ğŸ“¤ ******** å·¥ä½œæµæ¦‚è¦ ********")
                logger.info(f"ğŸ¯ å·¥ä½œæµ ID: {workflow_id}")
                logger.info(f"ğŸ¤– æ¨¡å‹: {model_display}")
                logger.info(f"âš™ï¸ é‡‡æ ·å™¨: {sampler_name} | è°ƒåº¦å™¨: {scheduler}")
                logger.info(f"ğŸ›ï¸ é‡ç»˜å¹…åº¦: {denoise} | æ­¥æ•°: {steps} | CFG: {cfg}")
                logger.info(f"ğŸ² ç§å­: {seed_info}")
                logger.info(f"ğŸ“Š èŠ‚ç‚¹æ€»æ•°: {total_nodes}")
                logger.info("ğŸ“¤ ***************************")

            except Exception as e:
                logger.warning(f"âš ï¸ å·¥ä½œæµæ¦‚è¦æ‰“å°å¤±è´¥: {e}")

        except Exception as e:
            logger.exception("âŒ å‚æ•°åˆå¹¶å¤±è´¥:")
            return jsonify({"code": 500, "msg": f"å‚æ•°åˆå¹¶å¤±è´¥: {str(e)}"}), 500

        try:
            result = proxy.send_to_comfyui(merged_workflow, client_id, comfyui_url)
            prompt_id = result["data"].get("prompt_id")

            if not prompt_id:
                logger.error("âŒ ComfyUI è¿”å›çš„ç»“æœä¸­æœªåŒ…å« prompt_id")
                return jsonify({"code": 500, "msg": "ComfyUIè¿”å›æ•°æ®å¼‚å¸¸"}), 500
            
            
            task_status[prompt_id] = {
                "type": "submitted",
                "data": {
                    "prompt_id": prompt_id,
                    "client_id": client_id,
                    "workflow_id": workflow_id,
                    "node_count": total_nodes  
                },
                "timestamp": time.time(),
                "enhanced": True
            }

           
            start_progress_tracker_by_mapping(prompt_id, workflow_id, client_id, comfyui_url)

            
            submit_message = {
                "type": "task_submitted",
                "data": {
                    "prompt_id": prompt_id,
                    "workflow_id": workflow_id,
                    "node_count": total_nodes,  
                    "client_id": client_id
                }
            }
            add_message_to_queue(client_id, submit_message)
            response_data = {
                "code": 0,
                "msg": "æäº¤æˆåŠŸ",
                "data": {
                    "prompt_id": prompt_id,
                    "taskId": prompt_id,
                    "number": result["data"].get("node_num", 0),
                    "client_id": client_id,
                    "node_num": total_nodes  # 
                }
            }
            
            #logger.info("âœ… ä»»åŠ¡æäº¤æˆåŠŸ")
            logger.info(f"  â†’ prompt_id: {prompt_id}")
            logger.info(f"  â†’ èŠ‚ç‚¹æ€»æ•°: {total_nodes}")
            
            return jsonify(response_data), 200

        except Exception as e:
            logger.exception("âŒ ComfyUIè¯·æ±‚å¤±è´¥:")
            return jsonify({"code": 500, "msg": f"ComfyUIè¯·æ±‚å¤±è´¥: {str(e)}"}), 500

    except Exception as e:
        logger.error(f"âŒ è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        return jsonify({"code": 500, "msg": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"}), 500


from flask import request
import json
import gevent
import time

@app.route("/ws")
def proxy_ws():
    ws = request.environ.get("wsgi.websocket")
    if not ws:
        logger.warning("âŒ æ’ä»¶å‘æ¥çš„æ˜¯æ™®é€š HTTP è¯·æ±‚ï¼Œé WebSocket")
        return "Expected WebSocket", 400

    client_id = request.args.get("clientId") or request.headers.get("Clientid")
    if not client_id:
        logger.warning("âŒ æœªæä¾› clientIdï¼Œæ— æ³•å»ºç«‹ä¼ª WebSocket è½®è¯¢")
        ws.close()
        return


    try:
        while True:
            msgs = get_messages_for_client(client_id)

            if not msgs:
                logger.debug(f"ğŸ• [client {client_id}] å½“å‰æ— æ–°æ¶ˆæ¯")
            else:
                for msg in msgs:
                    ws.send(json.dumps(msg))
                    logger.info(f"ğŸ“¤ [client {client_id}] å·²è½¬å‘æ¶ˆæ¯: {msg}")

            gevent.sleep(1)  
    except Exception as e:
        logger.warning(f"âš ï¸ WebSocket å¼‚å¸¸: {e}")
    finally:
        ws.close()

@app.route('/api/config/comfyui_url', methods=['POST'])
def update_comfyui_url():
    data = request.get_json() or {}
    url = data.get('url')
    if not url:
        return jsonify({"code": 400, "msg": "missing url"}), 400
    url = sanitize_url(url)
    proxy.config['comfyui_url'] = url
    proxy.save_config()
    global COMFYUI_URL
    COMFYUI_URL = url
    return jsonify({"code": 200, "msg": "updated", "data": {"comfyuiUrl": url}})

CLOUD_CHECK_URL = "https://umanage.lightcc.cloud/prod-api/psPlus/workflow/checkOnline"

@app.route('/psPlus/workflow/checkOnline', methods=['GET'])
def check_online():
    headers = dict(request.headers)
    token = headers.get('Authorization', '').replace('Bearer ', '')
    if token in sessions:
        logger.info("âœ… åœ¨çº¿æ£€æŸ¥é€šè¿‡ - local")
        return jsonify({"code": 200, "msg": "æ“ä½œæˆåŠŸ", "data": {"online": True}}), 200

    try:
        logger.info("ğŸ” [CheckOnline] æ”¶åˆ°è¯·æ±‚")
        logger.debug("[CheckOnline] Headers: %s", headers)

        response = requests.get(
            CLOUD_CHECK_URL,
            headers=headers,
            timeout=proxy.config.get("timeout", 30)
        )
        logger.debug("[CheckOnline] äº‘ç«¯å“åº”çŠ¶æ€ç : %s", response.status_code)
        logger.debug("[CheckOnline] äº‘ç«¯å“åº”å†…å®¹: %s", response.text)

        if response.status_code == 200:
            logger.info("âœ… åœ¨çº¿æ£€æŸ¥é€šè¿‡ - by cloud")
        else:
            logger.warning("âŒ åœ¨çº¿æ£€æŸ¥å¤±è´¥ - by cloud")

        return Response(
            response.content,
            status=response.status_code,
            content_type=response.headers.get('Content-Type', 'application/json')
        )

    except Exception as e:
        logger.error("[CheckOnline] è¯·æ±‚äº‘ç«¯å¤±è´¥: %s", str(e))
        return jsonify({"code": 500, "msg": "checkOnline failed"}), 500


@app.route('/health', methods=['GET'])
def health_check():

    return jsonify({
        "status": "healthy",
        "service": "huiying-proxy-enhanced-fixed",
        "version": "2.5.0",
        "timestamp": datetime.now().isoformat(),
        "features": ["http_polling", "task_status", "message_queue", "enhanced_progress", "upload_progress", "mask_support"]
    })


CLOUD_AUTH_URL = "https://umanage.lightcc.cloud/prod-api/auth/login"

CLOUD_LOGOUT_URL = "https://umanage.lightcc.cloud/prod-api/auth/logout"
@app.route('/auth/login', methods=['POST'])
def login_compatible():
    data = request.get_json(silent=True)
    if data is None:
        data = request.form.to_dict()
    username = data.get("username")
    password = data.get("password")

    logger.info("ğŸ”‘ [Login] æ”¶åˆ°ç™»å½•è¯·æ±‚ï¼Œç”¨æˆ·å: %s", username)

    user = LOCAL_USERS.get(username)
    if user:
        logger.info("[Login] åœ¨æœ¬åœ°ç”¨æˆ·åˆ—è¡¨ä¸­æ‰¾åˆ°ç”¨æˆ· %s", username)
    else:
        logger.info("[Login] æœ¬åœ°ç”¨æˆ·åˆ—è¡¨ä¸­æœªæ‰¾åˆ°ç”¨æˆ· %s", username)

    if user and user.get("password") == password:
        token = uuid.uuid4().hex
        sessions[token] = username
        logger.info("âœ… [Login] æœ¬åœ°è®¤è¯æˆåŠŸ")
        return jsonify({
            "code": 200,
            "msg": "æ“ä½œæˆåŠŸ",
            "data": {
                "scope": None,
                "openid": None,
                "access_token": token,
                "refresh_token": None,
                "expire_in": 604799,
                "refresh_expire_in": None,
                "client_id": data.get("clientId")
            }
        }), 200

    if user:
        logger.warning("[Login] æœ¬åœ°å¯†ç ä¸åŒ¹é…")
    else:
        logger.info("[Login] æœ¬åœ°è®¤è¯å¤±è´¥ï¼Œå°è¯•äº‘ç«¯ç™»å½•")

    try:
        response = requests.post(
            CLOUD_AUTH_URL,
            json=data,
            timeout=proxy.config.get("timeout", 30)
        )
        logger.info("[Login] äº‘ç«¯è¿”å›çŠ¶æ€ç : %s", response.status_code)
        logger.debug("[Login] äº‘ç«¯è¿”å›å†…å®¹: %s", response.text)

        if response.status_code == 200:
            logger.info("âœ… [Login] äº‘ç«¯ç™»å½•æˆåŠŸ")
        else:
            logger.warning("âŒ [Login] äº‘ç«¯ç™»å½•å¤±è´¥")

        return Response(
            response.content,
            status=response.status_code,
            content_type=response.headers.get('Content-Type', 'application/json')
        )
    except requests.RequestException as e:
        logger.error("[Login] äº‘ç«¯è¯·æ±‚å¼‚å¸¸: %s", str(e))
        return jsonify({"code": 1, "msg": "request to cloud failed"}), 500

    
@app.route('/auth/logout', methods=['POST'])
def logout_proxy():
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    logger.info("ğŸ”‘ [Logout] æ”¶åˆ°é€€å‡ºè¯·æ±‚ï¼Œtoken: %s", token)
    if token in sessions:
        user = sessions.pop(token)
        logger.info("âœ… [Logout] æœ¬åœ°é€€å‡ºæˆåŠŸ, ç”¨æˆ·: %s", user)
        return jsonify({"code": 200, "msg": "æ“ä½œæˆåŠŸ", "data": None}), 200
    else:
        logger.info("[Logout] æœ¬åœ°ä¼šè¯ä¸å­˜åœ¨ï¼Œå°è¯•äº‘ç«¯ç™»å‡º")

    try:
        payload = request.get_data()
        headers = {
            key: value for key, value in request.headers.items()
            if key.lower() != 'host'
        }
        response = requests.post(
            CLOUD_LOGOUT_URL,
            headers=headers,
            data=payload,
            timeout=proxy.config.get("timeout", 30)
        )
        logger.info("[Logout] äº‘ç«¯è¿”å›çŠ¶æ€ç : %s", response.status_code)
        logger.debug("[Logout] äº‘ç«¯è¿”å›å†…å®¹: %s", response.text)
        try:
            result = response.json()
        except Exception:
            result = {"code": 500, "msg": "äº‘ç«¯å“åº”æ ¼å¼é”™è¯¯", "raw": response.text}
        if response.status_code == 200:
            logger.info("âœ… [Logout] é€€å‡ºæˆåŠŸ - by cloud")
        else:
            logger.warning("âŒ [Logout] é€€å‡ºå¤±è´¥ - by cloud")
        return jsonify(result), response.status_code
    except Exception as e:
        logger.error("[Logout] äº‘ç«¯è¯·æ±‚å¼‚å¸¸: %s", str(e))
        return jsonify({"code": 500, "msg": "ä»£ç†ç™»å‡ºå¤±è´¥", "error": str(e)}), 500

from gevent.pywsgi import WSGIServer
from geventwebsocket.handler import WebSocketHandler
if __name__ == '__main__':

    # monkey.patch_all() has already been called at the top of the file
    import logging
    

    logger.info("ğŸ”§ å¯åŠ¨ ComfyUI WebSocket ç›‘å¬çº¿ç¨‹...")
    logger.info("ğŸ”„ å·²ä½¿ç”¨å¢å¼ºHTTPè½®è¯¢æ¨¡å¼")
    comfy_ws_listener()

    logger.info("ğŸ”§ å¯åŠ¨æ¸…ç†ä»»åŠ¡çº¿ç¨‹æœåŠ¡")
    Thread(target=cleanup_task, daemon=True).start()
    

    port = proxy.config.get('proxy_port', 8080)
    server = WSGIServer(
        ("0.0.0.0", port),
        app,
        handler_class=WebSocketHandler,
        log=None
    )
    logger.info("âœ… HTTP & WebSocket æœåŠ¡å¯åŠ¨æˆåŠŸ")
    logger.info(f"ğŸŸ¢ ä»£ç†æœåŠ¡å¯åŠ¨ï¼Œç›‘å¬ç«¯å£: {port}")
    logger.info(
        f"âœ… å·¥ä½œæµæ˜ å°„é…ç½®åŠ è½½å®Œæˆï¼Œå·¥ä½œæµæ•°é‡: {len(proxy.mappings.get('workflow_mappings', {}))}"
    )
    print("============== æ¬¢è¿ä½¿ç”¨ç»˜å½± AICG ä»£ç†ç»ˆç«¯æœåŠ¡ v2.5  ==============")

    server.serve_forever()

